name: Model Deployment CI/CD

on:
  push:
    paths:
      - 'mcp_server/model_serving.py'
      - 'mcp_server/model_registry.py'
      - 'mcp_server/model_versioning.py'
      - 'mcp_server/model_monitoring.py'
      - 'tests/test_model_serving.py'
      - 'tests/test_model_registry.py'
      - 'tests/test_model_versioning.py'
      - 'tests/test_model_monitoring.py'
      - '.github/workflows/model_deployment_ci.yml'
  pull_request:
    paths:
      - 'mcp_server/model_serving.py'
      - 'mcp_server/model_registry.py'
      - 'mcp_server/model_versioning.py'
      - 'mcp_server/model_monitoring.py'
      - 'tests/test_model_*.py'
  workflow_dispatch:
    inputs:
      run_integration_tests:
        description: 'Run integration tests with real MLflow server'
        required: false
        default: 'false'

jobs:
  test-model-deployment:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11']

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist pytest-timeout

    - name: Run Model Serving Tests
      run: |
        pytest tests/test_model_serving.py -v --cov=mcp_server.model_serving --cov-report=xml --cov-report=term
      timeout-minutes: 5

    - name: Run Model Registry Tests
      run: |
        pytest tests/test_model_registry.py -v --cov=mcp_server.model_registry --cov-report=xml --cov-report=term
      timeout-minutes: 5

    - name: Run Model Versioning Tests
      run: |
        pytest tests/test_model_versioning.py -v --cov=mcp_server.model_versioning --cov-report=xml --cov-report=term
      timeout-minutes: 5

    - name: Run Model Monitoring Tests
      run: |
        pytest tests/test_model_monitoring.py -v --cov=mcp_server.model_monitoring --cov-report=xml --cov-report=term
      timeout-minutes: 5

    - name: Run All Deployment Tests
      run: |
        pytest tests/test_model_*.py -v --cov=mcp_server --cov-report=xml --cov-report=html --cov-report=term-missing
      timeout-minutes: 10

    - name: Upload Coverage Reports
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.xml
        flags: model-deployment
        name: model-deployment-coverage

    - name: Generate Coverage Badge
      run: |
        pip install coverage-badge
        coverage-badge -o coverage.svg -f

    - name: Check Test Count
      run: |
        TEST_COUNT=$(pytest tests/test_model_*.py --collect-only -q | grep -c "test_")
        echo "Total tests: $TEST_COUNT"
        if [ $TEST_COUNT -lt 97 ]; then
          echo "::error::Expected at least 97 tests, found $TEST_COUNT"
          exit 1
        fi

    - name: Archive Test Results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          htmlcov/
          coverage.xml
          .coverage

  integration-tests:
    runs-on: ubuntu-latest
    if: github.event.inputs.run_integration_tests == 'true'

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest mlflow

    - name: Start MLflow Server
      run: |
        mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlflow-artifacts --host 0.0.0.0 --port 5000 &
        sleep 5
      env:
        MLFLOW_TRACKING_URI: http://localhost:5000

    - name: Run Integration Tests
      run: |
        pytest tests/test_model_*.py -v -m "not mock" --tb=short
      env:
        MLFLOW_TRACKING_URI: http://localhost:5000
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
      timeout-minutes: 15

  code-quality:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black mypy pylint

    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 mcp_server/model_*.py --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 mcp_server/model_*.py --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics

    - name: Check formatting with black
      run: |
        black --check mcp_server/model_*.py tests/test_model_*.py || true

    - name: Type checking with mypy
      run: |
        mypy mcp_server/model_serving.py --ignore-missing-imports || true
        mypy mcp_server/model_registry.py --ignore-missing-imports || true
        mypy mcp_server/model_versioning.py --ignore-missing-imports || true
        mypy mcp_server/model_monitoring.py --ignore-missing-imports || true

  performance-tests:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark

    - name: Run Performance Benchmarks
      run: |
        pytest tests/test_model_serving.py -v --benchmark-only || echo "No benchmarks found"

    - name: Stress Test - Concurrent Predictions
      run: |
        python -c "
        from mcp_server.model_serving import ModelServingManager
        import time
        import threading

        class MockModel:
            def predict(self, inputs):
                return [0.5] * len(inputs)

        manager = ModelServingManager(mock_mode=True)
        manager.deploy_model('test', 'v1.0', MockModel(), set_active=True)

        def make_predictions():
            for _ in range(1000):
                manager.predict('test', [{'x': 1}])

        threads = [threading.Thread(target=make_predictions) for _ in range(10)]
        start = time.time()
        for t in threads: t.start()
        for t in threads: t.join()
        elapsed = time.time() - start

        print(f'10,000 predictions in {elapsed:.2f}s ({10000/elapsed:.2f} pred/sec)')
        "

  security-scan:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit

    - name: Security scan with bandit
      run: |
        bandit -r mcp_server/model_*.py -f json -o bandit-report.json || true

    - name: Check dependencies with safety
      run: |
        pip freeze | safety check --json || true

    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: bandit-report.json

  notify:
    runs-on: ubuntu-latest
    needs: [test-model-deployment, code-quality]
    if: always()

    steps:
    - name: Check job status
      run: |
        if [ "${{ needs.test-model-deployment.result }}" == "success" ] && [ "${{ needs.code-quality.result }}" == "success" ]; then
          echo "✅ All checks passed!"
        else
          echo "❌ Some checks failed"
          exit 1
        fi
