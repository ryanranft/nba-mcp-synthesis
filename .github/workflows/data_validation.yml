name: Data Validation Pipeline

on:
  schedule:
    # Run daily at 2 AM UTC (after data load)
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      dataset_path:
        description: 'Path to dataset to validate'
        required: false
        default: 'data/'
      enable_profiling:
        description: 'Enable data profiling'
        required: false
        default: 'true'
      enable_drift_detection:
        description: 'Enable drift detection'
        required: false
        default: 'true'

jobs:
  validate-nba-data:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install great-expectations scikit-learn scipy

    - name: Run data validation pipeline
      run: |
        python -c "
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path.cwd()))

        from mcp_server.data_validation_pipeline import (
            DataValidationPipeline,
            PipelineConfig,
        )
        from mcp_server.data_profiler import DataProfiler
        from mcp_server.integrity_checker import IntegrityChecker
        import pandas as pd
        import numpy as np
        import json
        from datetime import datetime

        # Create sample data (in production, load from ${{ github.event.inputs.dataset_path || 'data/' }})
        np.random.seed(42)
        player_data = pd.DataFrame({
            'player_id': range(1, 101),
            'player_name': [f'Player{i}' for i in range(1, 101)],
            'ppg': np.random.normal(20, 5, 100),
            'rpg': np.random.normal(7, 2, 100),
            'apg': np.random.normal(5, 2, 100),
            'fg_pct': np.random.uniform(0.4, 0.6, 100),
            'games_played': np.random.randint(60, 82, 100),
        })

        # Configure pipeline
        config = PipelineConfig(
            enable_schema_validation=True,
            enable_quality_check=True,
            enable_business_rules=True,
            enable_profiling='${{ github.event.inputs.enable_profiling }}' == 'true',
            min_quality_score=0.9,
            save_results=True,
        )

        # Run validation pipeline
        pipeline = DataValidationPipeline(config=config)

        schema = {
            'columns': ['player_id', 'player_name', 'ppg', 'rpg', 'apg', 'fg_pct', 'games_played'],
            'types': {
                'player_id': 'int64',
                'player_name': 'object',
                'ppg': 'float64',
                'rpg': 'float64',
                'apg': 'float64',
                'fg_pct': 'float64',
                'games_played': 'int64',
            }
        }

        result = pipeline.validate(player_data, 'nba_player_stats', schema=schema)

        print(f'\\n=== Validation Pipeline Results ===')
        print(f'Pipeline ID: {result.pipeline_id}')
        print(f'Dataset: {result.dataset_name}')
        print(f'Stage: {result.current_stage.value}')
        print(f'Passed: {result.passed}')
        print(f'Duration: {result.duration_seconds:.2f}s')
        print(f'Total Issues: {len(result.issues)}')
        print(f'Critical Issues: {len(result.critical_issues)}')

        # Save detailed results
        results_file = Path('validation_results') / f'{result.pipeline_id}_detailed.json'
        results_file.parent.mkdir(exist_ok=True)
        with open(results_file, 'w') as f:
            json.dump(result.to_dict(), f, indent=2, default=str)

        print(f'\\nDetailed results saved to: {results_file}')

        # Exit with error if validation failed
        if not result.passed:
            print('\\n❌ Validation failed!')
            sys.exit(1)
        else:
            print('\\n✅ Validation passed!')
        "

    - name: Run data profiling
      if: github.event.inputs.enable_profiling == 'true' || github.event_name == 'schedule'
      run: |
        python -c "
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path.cwd()))

        from mcp_server.data_profiler import DataProfiler
        import pandas as pd
        import numpy as np
        import json

        # Create sample data
        np.random.seed(42)
        player_data = pd.DataFrame({
            'player_id': range(1, 101),
            'ppg': np.random.normal(20, 5, 100),
            'rpg': np.random.normal(7, 2, 100),
            'apg': np.random.normal(5, 2, 100),
            'fg_pct': np.random.uniform(0.4, 0.6, 100),
            'games_played': np.random.randint(60, 82, 100),
        })

        # Profile data
        profiler = DataProfiler()
        profile = profiler.profile_nba_player_stats(player_data)

        print(f'\\n=== Data Profiling Results ===')
        print(f'Dataset: {profile.dataset_name}')
        print(f'Rows: {profile.row_count}')
        print(f'Columns: {profile.column_count}')
        print(f'Memory: {profile.memory_usage_mb:.2f} MB')
        print(f'Quality Score: {profile.quality_score:.2%}')

        # Save profile
        profile_file = Path('validation_results') / f'profile_{profile.dataset_name}.json'
        profile_file.parent.mkdir(exist_ok=True)
        with open(profile_file, 'w') as f:
            json.dump(profile.to_dict(), f, indent=2, default=str)

        print(f'\\nProfile saved to: {profile_file}')
        "

    - name: Run drift detection
      if: github.event.inputs.enable_drift_detection == 'true' || github.event_name == 'schedule'
      run: |
        python -c "
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path.cwd()))

        from mcp_server.data_profiler import DataProfiler, DriftMethod
        import pandas as pd
        import numpy as np

        # Create reference and current data
        np.random.seed(42)
        reference_data = pd.DataFrame({
            'ppg': np.random.normal(20, 5, 1000),
            'rpg': np.random.normal(7, 2, 1000),
        })

        # Current data with slight drift
        np.random.seed(43)
        current_data = pd.DataFrame({
            'ppg': np.random.normal(20.5, 5, 1000),  # Slight drift
            'rpg': np.random.normal(7, 2, 1000),
        })

        # Detect drift
        profiler = DataProfiler()
        drift_results = profiler.detect_drift(
            reference_data,
            current_data,
            columns=['ppg', 'rpg'],
            method=DriftMethod.KL_DIVERGENCE,
        )

        print(f'\\n=== Drift Detection Results ===')
        for result in drift_results:
            print(f'Column: {result.column}')
            print(f'  Method: {result.method.value}')
            print(f'  Drift Score: {result.drift_score:.4f}')
            print(f'  Drift Detected: {result.drift_detected}')

        # Alert if drift detected
        if any(r.drift_detected for r in drift_results):
            print('\\n⚠️  Data drift detected! Review required.')
        else:
            print('\\n✅ No significant drift detected.')
        "

    - name: Run integrity checks
      run: |
        python -c "
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path.cwd()))

        from mcp_server.integrity_checker import IntegrityChecker
        import pandas as pd
        import numpy as np
        import json

        # Create sample data
        np.random.seed(42)
        player_data = pd.DataFrame({
            'player_id': range(1, 101),
            'ppg': np.random.normal(20, 5, 100),
            'games_played': np.random.randint(60, 82, 100),
        })

        # Run integrity checks
        checker = IntegrityChecker()
        report = checker.check(player_data, 'player_data', checks=['nba_player'])

        print(f'\\n=== Integrity Check Results ===')
        print(f'Dataset: {report.dataset_name}')
        print(f'Total Checks: {report.total_checks}')
        print(f'Violations: {report.violation_count}')
        print(f'Passed: {report.passed}')

        if report.violations:
            print(f'\\nViolation Summary:')
            for vtype, count in report.violation_types.items():
                print(f'  {vtype}: {count}')

        # Save report
        report_file = Path('validation_results') / f'integrity_report.json'
        report_file.parent.mkdir(exist_ok=True)
        with open(report_file, 'w') as f:
            json.dump(report.to_dict(), f, indent=2, default=str)

        print(f'\\nReport saved to: {report_file}')

        if not report.passed:
            print('\\n⚠️  Integrity violations detected!')
        "

    - name: Upload validation results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: validation-results
        path: validation_results/
        retention-days: 30

    - name: Generate summary report
      if: always()
      run: |
        echo "# 📊 Data Validation Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Run Information" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Validation Results" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Validation Pipeline: Completed" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Data Profiling: Completed" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Drift Detection: Completed" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Integrity Checks: Completed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "Validation results have been uploaded as artifacts and are available for 30 days." >> $GITHUB_STEP_SUMMARY

    - name: Notify on failure
      if: failure()
      run: |
        echo "::error::Data validation failed! Please review the validation results."
