# ğŸš€ Quick Reference - Three-Way Ollama Deployment

## âš¡ Quick Commands

```bash
# Check status of all deployments
./check_all_ollama.sh

# Switch web interface to local
./switch_to_local.sh

# Switch web interface to AWS
./switch_to_aws.sh

# Start web chat
./start_ollama_chat.sh

# AWS instance control
aws ec2 stop-instances --instance-ids i-06ec10b8f8fbe0ee4    # Stop (save $)
aws ec2 start-instances --instance-ids i-06ec10b8f8fbe0ee4   # Start
```

---

## ğŸ“ Three Deployments

| Location | URL | Models | Cost |
|----------|-----|--------|------|
| ğŸ  Local | localhost:11434 | Qwen 32B, GPT-OSS 20B | FREE |
| â˜ï¸ AWS | 34.226.246.126:11434 | Qwen 32B | $0.33/hr |
| âš™ï¸ Cursor | (all above) | All 3 models | - |

---

## ğŸ¯ Which to Use?

- **Quick query** â†’ ğŸ  Local GPT-OSS 20B (fastest)
- **Normal work** â†’ ğŸ  Local Qwen 32B (free, private)
- **Long analysis** â†’ â˜ï¸ AWS Qwen 32B (reliable, powerful)

---

## ğŸ”§ In Cursor

1. **Reload window:** `Cmd+Shift+P` â†’ "Reload Window"
2. **Click model selector** (bottom right)
3. **Choose:**
   - ğŸ  Local Ollama Qwen2.5-Coder 32B
   - â˜ï¸ AWS Ollama Qwen2.5-Coder 32B
   - ğŸ  Local Ollama GPT-OSS 20B

---

## ğŸ’° Daily Cost Estimate

**If you leave AWS running 24/7:** $7.92/day

**Smart usage (8 hours/day):** $2.64/day

**Use local + stop AWS when done:** $0/day! ğŸ‰

---

## ğŸ§ª Test Commands

```bash
# Test local
curl http://localhost:11434/api/tags

# Test AWS
curl http://34.226.246.126:11434/api/tags

# Quick chat test
curl http://localhost:11434/api/generate \
  -d '{"model":"qwen2.5-coder:32b","prompt":"Hi","stream":false}'
```

---

## ğŸ“š Full Documentation

- `THREE_WAY_DEPLOYMENT_COMPLETE.md` - Complete guide
- `DEPLOYMENT_SUCCESS.md` - AWS details
- `OLLAMA_MCP_SETUP_GUIDE.md` - Local setup
- `AWS_OLLAMA_GUIDE.md` - Cloud setup

---

## ğŸ†˜ Troubleshooting

**Issue:** Cursor doesn't show model options
**Fix:** Reload window (`Cmd+Shift+P` â†’ "Reload Window")

**Issue:** AWS not responding
**Fix:** Start instance: `aws ec2 start-instances --instance-ids i-06ec10b8f8fbe0ee4`

**Issue:** Local Ollama not running
**Fix:** Click Ollama icon in menu bar â†’ check it's running

---

## ğŸ‰ You're All Set!

**3 locations âœ…**
**3 models âœ…**
**100+ NBA tools âœ…**
**Full control âœ…**

**Happy coding! ğŸ€ğŸ¤–**


