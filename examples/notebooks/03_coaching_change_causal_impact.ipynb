{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coaching Change Causal Impact Analysis\n",
    "\n",
    "**Objective**: Measure the true causal effect of coaching changes on team performance\n",
    "\n",
    "**Methods Used**:\n",
    "- Propensity Score Matching (PSM)\n",
    "- Instrumental Variables (IV / 2SLS)\n",
    "- Regression Discontinuity Design (RDD)\n",
    "- Synthetic Control\n",
    "- Sensitivity Analysis (Rosenbaum bounds)\n",
    "\n",
    "**Causal Question**: Does firing a coach and hiring a new one actually improve team performance?\n",
    "\n",
    "**Challenge**: Teams that fire coaches are different from those that don't (confounding!)\n",
    "- Losing teams more likely to fire coaches\n",
    "- Simply comparing winners vs. losers biased\n",
    "- Need causal methods to isolate coaching effect\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Import NBA MCP causal inference tools\n",
    "from mcp_server.causal_inference import CausalInferenceAnalyzer\n",
    "from mcp_server.econometric_suite import EconometricSuite\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set1')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic coaching change data\n",
    "# In production, load from team database\n",
    "\n",
    "np.random.seed(42)\n",
    "n_teams = 120  # 30 teams over 4 seasons\n",
    "\n",
    "# Team characteristics (confounders)\n",
    "team_ids = [f'Team_{i}' for i in range(30)] * 4\n",
    "seasons = sorted([2020, 2021, 2022, 2023] * 30)\n",
    "\n",
    "# Previous season performance (key confounder)\n",
    "prev_wins = np.random.uniform(20, 62, n_teams)\n",
    "\n",
    "# Team quality (unobserved - creates endogeneity)\n",
    "team_quality = np.random.normal(0, 10, n_teams)\n",
    "\n",
    "# Roster talent\n",
    "roster_talent = np.random.normal(50, 15, n_teams)\n",
    "\n",
    "# Market size\n",
    "market_size = np.random.choice(['small', 'medium', 'large'], n_teams, p=[0.4, 0.4, 0.2])\n",
    "market_numeric = np.where(market_size == 'small', 0,\n",
    "                          np.where(market_size == 'medium', 1, 2))\n",
    "\n",
    "# Playoff last year\n",
    "made_playoffs_prev = (prev_wins >= 42).astype(int)\n",
    "\n",
    "# Treatment assignment (coaching change)\n",
    "# More likely if:\n",
    "# - Poor previous performance\n",
    "# - Large market (more pressure)\n",
    "# - Missed playoffs\n",
    "\n",
    "treatment_propensity = (\n",
    "    0.05  # Base probability\n",
    "    + 0.015 * (62 - prev_wins)  # Lower wins ‚Üí higher prob\n",
    "    + 0.10 * (1 - made_playoffs_prev)  # Missed playoffs\n",
    "    + 0.05 * market_numeric  # Larger market\n",
    ")\n",
    "treatment_propensity = np.clip(treatment_propensity, 0.05, 0.80)\n",
    "\n",
    "coaching_change = np.random.binomial(1, treatment_propensity, n_teams)\n",
    "\n",
    "# Outcome: Current season wins\n",
    "# True coaching effect: +3 wins on average (our ground truth)\n",
    "true_treatment_effect = 3.0\n",
    "\n",
    "# Regression to mean: bad teams improve, good teams decline\n",
    "regression_to_mean = 0.3 * (41 - prev_wins)\n",
    "\n",
    "# Generate outcome\n",
    "current_wins = (\n",
    "    prev_wins  # Baseline\n",
    "    + regression_to_mean  # Natural reversion\n",
    "    + 0.3 * roster_talent  # Talent effect\n",
    "    + 0.2 * team_quality  # Unobserved quality\n",
    "    + true_treatment_effect * coaching_change  # CAUSAL EFFECT\n",
    "    + np.random.normal(0, 5, n_teams)  # Random variation\n",
    ")\n",
    "current_wins = np.clip(current_wins, 10, 72)\n",
    "\n",
    "# Create DataFrame\n",
    "coaching_df = pd.DataFrame({\n",
    "    'team_id': team_ids,\n",
    "    'season': seasons,\n",
    "    'coaching_change': coaching_change,\n",
    "    'prev_wins': prev_wins,\n",
    "    'current_wins': current_wins,\n",
    "    'roster_talent': roster_talent,\n",
    "    'market_size': market_size,\n",
    "    'made_playoffs_prev': made_playoffs_prev,\n",
    "    'win_improvement': current_wins - prev_wins\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {coaching_df.shape}\")\n",
    "print(f\"\\nCoaching changes: {coaching_change.sum()} / {n_teams} ({100*coaching_change.mean():.1f}%)\")\n",
    "print(f\"True causal effect (ground truth): +{true_treatment_effect:.1f} wins\")\n",
    "coaching_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive comparison (BIASED - DO NOT USE)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NAIVE COMPARISON (BIASED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "changed = coaching_df[coaching_df['coaching_change'] == 1]['win_improvement'].mean()\n",
    "no_change = coaching_df[coaching_df['coaching_change'] == 0]['win_improvement'].mean()\n",
    "naive_effect = changed - no_change\n",
    "\n",
    "print(f\"\\nWin improvement (changed coach): {changed:.2f}\")\n",
    "print(f\"Win improvement (no change): {no_change:.2f}\")\n",
    "print(f\"\\nNaive effect estimate: {naive_effect:.2f} wins\")\n",
    "print(f\"True effect: {true_treatment_effect:.2f} wins\")\n",
    "print(f\"\\n‚ö†Ô∏è  BIAS: {naive_effect - true_treatment_effect:.2f} wins\")\n",
    "print(\"\\nWhy biased?\")\n",
    "print(\"- Teams that change coaches had worse previous records\")\n",
    "print(\"- They would have improved anyway (regression to mean)\")\n",
    "print(\"- We're not comparing apples-to-apples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize selection bias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Previous wins distribution\n",
    "axes[0].hist(coaching_df[coaching_df['coaching_change'] == 1]['prev_wins'], \n",
    "             bins=15, alpha=0.6, label='Changed Coach', color='red')\n",
    "axes[0].hist(coaching_df[coaching_df['coaching_change'] == 0]['prev_wins'], \n",
    "             bins=15, alpha=0.6, label='No Change', color='blue')\n",
    "axes[0].set_xlabel('Previous Season Wins', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Selection Bias: Who Gets Fired?', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Outcome by treatment\n",
    "axes[1].scatter(coaching_df['prev_wins'], coaching_df['current_wins'], \n",
    "                c=coaching_df['coaching_change'], cmap='RdBu_r', \n",
    "                alpha=0.6, s=60, edgecolors='black', linewidth=0.5)\n",
    "axes[1].plot([20, 62], [20, 62], 'k--', alpha=0.4, label='No Change Line')\n",
    "axes[1].set_xlabel('Previous Season Wins', fontsize=12)\n",
    "axes[1].set_ylabel('Current Season Wins', fontsize=12)\n",
    "axes[1].set_title('Outcome Pattern', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(['No improvement', 'No Change', 'Changed Coach'], fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Propensity Score Matching (PSM)\n",
    "\n",
    "**Idea**: Match each treated team (coaching change) with a similar control team (no change)\n",
    "\n",
    "**Steps**:\n",
    "1. Estimate propensity scores: P(Treatment | Covariates)\n",
    "2. Match treated to control based on similar propensity\n",
    "3. Compare outcomes only among matched pairs\n",
    "\n",
    "**Advantage**: Creates \"quasi-randomized\" comparison\n",
    "**Limitation**: Only controls for observed confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize causal inference analyzer\n",
    "causal = CausalInferenceAnalyzer(\n",
    "    data=coaching_df,\n",
    "    treatment_col='coaching_change',\n",
    "    outcome_col='current_wins'\n",
    ")\n",
    "\n",
    "# Run propensity score matching\n",
    "psm_result = causal.propensity_score_matching(\n",
    "    covariates=['prev_wins', 'roster_talent', 'made_playoffs_prev'],\n",
    "    matching_method='nearest',\n",
    "    caliper=0.1  # Maximum propensity score difference\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROPENSITY SCORE MATCHING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMatched pairs: {psm_result.n_matched}\")\n",
    "print(f\"Unmatched treated: {psm_result.n_unmatched_treated}\")\n",
    "print(f\"Unmatched control: {psm_result.n_unmatched_control}\")\n",
    "print(f\"\\nAverage Treatment Effect (ATE): {psm_result.ate:.2f} wins\")\n",
    "print(f\"Standard Error: {psm_result.se:.2f}\")\n",
    "print(f\"95% CI: [{psm_result.ci_lower:.2f}, {psm_result.ci_upper:.2f}]\")\n",
    "print(f\"P-value: {psm_result.p_value:.4f}\")\n",
    "\n",
    "if psm_result.p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ Statistically significant at 5% level\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Not statistically significant\")\n",
    "\n",
    "print(f\"\\nComparison to Truth:\")\n",
    "print(f\"  Estimated effect: {psm_result.ate:.2f} wins\")\n",
    "print(f\"  True effect: {true_treatment_effect:.2f} wins\")\n",
    "print(f\"  Estimation error: {abs(psm_result.ate - true_treatment_effect):.2f} wins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check covariate balance before/after matching\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COVARIATE BALANCE CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "balance = psm_result.balance_diagnostics\n",
    "balance_df = pd.DataFrame(balance).T\n",
    "print(\"\\n\", balance_df.to_string())\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Standardized mean difference (SMD) < 0.1 = good balance\")\n",
    "print(\"- After matching, treated and control groups should be similar\")\n",
    "print(\"- This validates causal interpretation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize propensity score distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before matching\n",
    "treated_ps = psm_result.propensity_scores[coaching_df['coaching_change'] == 1]\n",
    "control_ps = psm_result.propensity_scores[coaching_df['coaching_change'] == 0]\n",
    "\n",
    "axes[0].hist(treated_ps, bins=20, alpha=0.6, label='Treated', color='red')\n",
    "axes[0].hist(control_ps, bins=20, alpha=0.6, label='Control', color='blue')\n",
    "axes[0].set_xlabel('Propensity Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Before Matching', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# After matching (only matched pairs)\n",
    "matched_treated_ps = psm_result.matched_propensity_scores['treated']\n",
    "matched_control_ps = psm_result.matched_propensity_scores['control']\n",
    "\n",
    "axes[1].hist(matched_treated_ps, bins=15, alpha=0.6, label='Treated', color='red')\n",
    "axes[1].hist(matched_control_ps, bins=15, alpha=0.6, label='Control', color='blue')\n",
    "axes[1].set_xlabel('Propensity Score', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('After Matching (Balanced)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAfter matching, distributions overlap ‚Üí valid comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Instrumental Variables (IV / 2SLS)\n",
    "\n",
    "**Problem**: What if we have unobserved confounders?\n",
    "- Team culture, front office quality, etc.\n",
    "- PSM can't control for these\n",
    "\n",
    "**Solution**: Find an **instrument** - a variable that:\n",
    "1. ‚úÖ Affects treatment (coaching change)\n",
    "2. ‚úÖ Does NOT directly affect outcome (only through treatment)\n",
    "3. ‚úÖ Uncorrelated with unobserved confounders\n",
    "\n",
    "**Example Instrument**: Contract expiration\n",
    "- Coaches on expiring contracts more likely to be replaced\n",
    "- Contract timing arbitrary (not related to current performance)\n",
    "- Satisfies IV assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add instrument: contract expiration (exogenous shock)\n",
    "# Randomly assign 30% of teams to have expiring contracts\n",
    "contract_expiring = np.random.binomial(1, 0.3, n_teams)\n",
    "coaching_df['contract_expiring'] = contract_expiring\n",
    "\n",
    "# Instrument increases coaching change probability\n",
    "# (but doesn't directly affect wins)\n",
    "\n",
    "# Run IV regression (2SLS)\n",
    "iv_result = causal.instrumental_variables(\n",
    "    instruments='contract_expiring',\n",
    "    covariates=['prev_wins', 'roster_talent']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTRUMENTAL VARIABLES (2SLS)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nInstrument: Contract Expiring\")\n",
    "print(f\"\\nFirst Stage (Treatment ~ Instrument):\")\n",
    "print(f\"  F-statistic: {iv_result.first_stage_f_stat:.2f}\")\n",
    "if iv_result.first_stage_f_stat > 10:\n",
    "    print(f\"  ‚úÖ Strong instrument (F > 10)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Weak instrument (F < 10) - results unreliable\")\n",
    "\n",
    "print(f\"\\nSecond Stage (Outcome ~ Predicted Treatment):\")\n",
    "print(f\"  Treatment effect: {iv_result.treatment_effect:.2f} wins\")\n",
    "print(f\"  Standard error: {iv_result.se:.2f}\")\n",
    "print(f\"  95% CI: [{iv_result.ci_lower:.2f}, {iv_result.ci_upper:.2f}]\")\n",
    "print(f\"  P-value: {iv_result.p_value:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  IV estimate: {iv_result.treatment_effect:.2f} wins\")\n",
    "print(f\"  PSM estimate: {psm_result.ate:.2f} wins\")\n",
    "print(f\"  True effect: {true_treatment_effect:.2f} wins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Regression Discontinuity Design (RDD)\n",
    "\n",
    "**Idea**: Exploit a cutoff rule for treatment assignment\n",
    "\n",
    "**Example**: Teams below 0.500 (41 wins) much more likely to fire coach\n",
    "- Compare teams just above vs. just below cutoff\n",
    "- These teams are very similar except coaching decision\n",
    "- Provides quasi-experimental estimate\n",
    "\n",
    "**Assumption**: No manipulation of running variable around cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regression discontinuity\n",
    "rdd_result = causal.regression_discontinuity(\n",
    "    running_var='prev_wins',\n",
    "    cutoff=41,  # 0.500 winning percentage\n",
    "    bandwidth=10,  # Use teams within 10 wins of cutoff\n",
    "    polynomial_order=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGRESSION DISCONTINUITY DESIGN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCutoff: {rdd_result.cutoff} wins (0.500)\")\n",
    "print(f\"Bandwidth: {rdd_result.bandwidth} wins\")\n",
    "print(f\"\\nSample sizes:\")\n",
    "print(f\"  Below cutoff: {rdd_result.n_below}\")\n",
    "print(f\"  Above cutoff: {rdd_result.n_above}\")\n",
    "\n",
    "print(f\"\\nTreatment Effect at Cutoff: {rdd_result.treatment_effect:.2f} wins\")\n",
    "print(f\"Standard Error: {rdd_result.se:.2f}\")\n",
    "print(f\"95% CI: [{rdd_result.ci_lower:.2f}, {rdd_result.ci_upper:.2f}]\")\n",
    "print(f\"P-value: {rdd_result.p_value:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- Teams just below .500 who changed coaches improved by {rdd_result.treatment_effect:.1f} wins\")\n",
    "print(f\"- This is the Local Average Treatment Effect (LATE) at the cutoff\")\n",
    "print(f\"- May not generalize to teams far from cutoff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RDD\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot raw data\n",
    "below = coaching_df[coaching_df['prev_wins'] < 41]\n",
    "above = coaching_df[coaching_df['prev_wins'] >= 41]\n",
    "\n",
    "ax.scatter(below['prev_wins'], below['current_wins'], \n",
    "           c=below['coaching_change'], cmap='RdBu_r', alpha=0.6, s=80, \n",
    "           edgecolors='black', linewidth=0.5, label='Below .500')\n",
    "ax.scatter(above['prev_wins'], above['current_wins'], \n",
    "           c=above['coaching_change'], cmap='RdBu_r', alpha=0.6, s=80, \n",
    "           marker='s', edgecolors='black', linewidth=0.5, label='Above .500')\n",
    "\n",
    "# Add fitted lines\n",
    "# Below cutoff\n",
    "below_x = np.linspace(below['prev_wins'].min(), 41, 100)\n",
    "below_y = rdd_result.coefficients['below']['intercept'] + rdd_result.coefficients['below']['slope'] * below_x\n",
    "ax.plot(below_x, below_y, 'r-', linewidth=3, label='Trend (Below)')\n",
    "\n",
    "# Above cutoff\n",
    "above_x = np.linspace(41, above['prev_wins'].max(), 100)\n",
    "above_y = rdd_result.coefficients['above']['intercept'] + rdd_result.coefficients['above']['slope'] * above_x\n",
    "ax.plot(above_x, above_y, 'b-', linewidth=3, label='Trend (Above)')\n",
    "\n",
    "# Mark cutoff\n",
    "ax.axvline(x=41, color='black', linestyle='--', linewidth=2, alpha=0.7, label='Cutoff (41 wins)')\n",
    "\n",
    "# Annotate discontinuity\n",
    "jump = rdd_result.treatment_effect\n",
    "ax.annotate(f'Jump = {jump:.1f} wins', \n",
    "            xy=(41, 45), xytext=(35, 55),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='green'),\n",
    "            fontsize=13, fontweight='bold', color='green')\n",
    "\n",
    "ax.set_xlabel('Previous Season Wins', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Current Season Wins', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Regression Discontinuity: Coaching Change Effect', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Synthetic Control\n",
    "\n",
    "**Use Case**: Single treated unit (e.g., Lakers fire coach mid-season)\n",
    "\n",
    "**Idea**: Create a \"synthetic Lakers\" from weighted average of control teams\n",
    "- Match pre-treatment characteristics\n",
    "- Compare post-treatment outcomes\n",
    "- Difference = causal effect\n",
    "\n",
    "**Advantage**: Visual, intuitive, handles single treatment case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data for synthetic control\n",
    "# Simulate one team (Lakers) changing coach in season 2022\n",
    "\n",
    "np.random.seed(42)\n",
    "years = np.arange(2018, 2025)\n",
    "n_years = len(years)\n",
    "\n",
    "# Lakers performance (treated unit)\n",
    "lakers_pre = [45, 47, 42, 41]  # 2018-2021\n",
    "lakers_post_counterfactual = [43, 44, 45]  # What would have happened\n",
    "lakers_post_actual = [47, 50, 51]  # What actually happened (+treatment effect)\n",
    "lakers = lakers_pre + lakers_post_actual\n",
    "\n",
    "# Control teams (to construct synthetic control)\n",
    "control_teams = {\n",
    "    'Team_A': [46, 48, 41, 40, 42, 43, 44],\n",
    "    'Team_B': [44, 45, 43, 42, 44, 45, 46],\n",
    "    'Team_C': [47, 46, 44, 43, 45, 46, 47],\n",
    "}\n",
    "\n",
    "# Run synthetic control\n",
    "sc_result = causal.synthetic_control(\n",
    "    treated_unit='Lakers',\n",
    "    treated_data=lakers[:4],  # Pre-treatment only\n",
    "    control_data=control_teams,\n",
    "    treatment_time=4  # Index where treatment starts\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SYNTHETIC CONTROL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTreated Unit: Lakers\")\n",
    "print(f\"Treatment Time: 2022 (Season 5)\")\n",
    "print(f\"\\nSynthetic Control Weights:\")\n",
    "for team, weight in sc_result.weights.items():\n",
    "    print(f\"  {team}: {weight:.3f}\")\n",
    "\n",
    "print(f\"\\nPre-treatment fit (RMSE): {sc_result.pre_treatment_rmse:.2f}\")\n",
    "print(f\"\\nPost-treatment effects:\")\n",
    "for year, effect in sc_result.treatment_effects.items():\n",
    "    print(f\"  Year {year}: +{effect:.1f} wins\")\n",
    "\n",
    "print(f\"\\nAverage Treatment Effect: +{sc_result.average_effect:.1f} wins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthetic control\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Treated vs. Synthetic\n",
    "axes[0].plot(years, lakers, 'o-', linewidth=2.5, markersize=8, \n",
    "             color='red', label='Lakers (Actual)', zorder=3)\n",
    "axes[0].plot(years, sc_result.synthetic_path, 's--', linewidth=2.5, markersize=8,\n",
    "             color='blue', label='Synthetic Lakers', alpha=0.8, zorder=2)\n",
    "axes[0].axvline(x=2021.5, color='black', linestyle=':', linewidth=2, \n",
    "                label='Coaching Change', alpha=0.7)\n",
    "axes[0].fill_between([2022, 2024], 35, 55, alpha=0.15, color='yellow', \n",
    "                      label='Post-Treatment Period')\n",
    "axes[0].set_ylabel('Wins', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Synthetic Control: Lakers Coaching Change', fontsize=15, fontweight='bold')\n",
    "axes[0].legend(fontsize=11, loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([35, 55])\n",
    "\n",
    "# Plot 2: Treatment Effect Over Time\n",
    "post_years = years[4:]\n",
    "effects = [sc_result.treatment_effects[y] for y in post_years]\n",
    "axes[1].bar(post_years, effects, color='green', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1].axhline(y=true_treatment_effect, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'True Effect ({true_treatment_effect:.0f} wins)')\n",
    "axes[1].set_xlabel('Season', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Treatment Effect (Wins)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_title('Estimated Coaching Effect by Year', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5: Sensitivity Analysis\n",
    "\n",
    "**Question**: How robust are our results to unobserved confounding?\n",
    "\n",
    "**Rosenbaum Bounds**:\n",
    "- Asks: How strong would hidden bias need to be to overturn our conclusion?\n",
    "- **Gamma (Œì)**: Sensitivity parameter\n",
    "  - Œì = 1: No hidden bias\n",
    "  - Œì = 2: Matched pairs differ 2x in odds of treatment\n",
    "  - Œì = 5: 5x difference in odds\n",
    "\n",
    "**Interpretation**:\n",
    "- If result robust to Œì = 3+ ‚Üí strong evidence\n",
    "- If sensitive to Œì = 1.5 ‚Üí weak evidence, hidden bias concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sensitivity analysis on PSM results\n",
    "sensitivity_result = causal.sensitivity_analysis(\n",
    "    psm_result=psm_result,\n",
    "    gamma_range=[1.0, 1.5, 2.0, 2.5, 3.0]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENSITIVITY ANALYSIS (Rosenbaum Bounds)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nHow robust is our PSM result to hidden bias?\\n\")\n",
    "\n",
    "sensitivity_df = pd.DataFrame(sensitivity_result).T\n",
    "print(sensitivity_df.to_string())\n",
    "\n",
    "print(\"\\n\\nInterpretation:\")\n",
    "print(\"- Œì = 1.0: No hidden bias (our baseline assumption)\")\n",
    "print(\"- Œì = 2.0: Hidden confounder could double treatment odds\")\n",
    "print(\"- Œì = 3.0: 3x difference in treatment odds\")\n",
    "print(\"\\nIf p-value stays < 0.05 up to Œì = 2.5:\")\n",
    "print(\"  ‚Üí Result is ROBUST to moderate hidden bias\")\n",
    "print(\"  ‚Üí Coaching effect likely real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "gammas = list(sensitivity_result.keys())\n",
    "p_values = [sensitivity_result[g]['p_value'] for g in gammas]\n",
    "\n",
    "ax.plot(gammas, p_values, 'o-', linewidth=3, markersize=10, color='darkblue')\n",
    "ax.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='Significance Threshold (Œ±=0.05)')\n",
    "ax.fill_between(gammas, 0, 0.05, alpha=0.2, color='red', label='Reject Null')\n",
    "ax.fill_between(gammas, 0.05, 1, alpha=0.2, color='gray', label='Fail to Reject')\n",
    "\n",
    "ax.set_xlabel('Gamma (Œì) - Hidden Bias Strength', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('P-value', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Sensitivity to Unobserved Confounding', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 0.20])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find critical Gamma\n",
    "critical_gamma = None\n",
    "for g in gammas:\n",
    "    if sensitivity_result[g]['p_value'] > 0.05:\n",
    "        critical_gamma = g\n",
    "        break\n",
    "\n",
    "if critical_gamma:\n",
    "    print(f\"\\nCritical Œì: {critical_gamma:.1f}\")\n",
    "    print(f\"Results become non-significant at Œì = {critical_gamma:.1f}\")\n",
    "else:\n",
    "    print(f\"\\nResults remain significant even at Œì = {max(gammas):.1f}\")\n",
    "    print(f\"Very robust to hidden bias!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Method Comparison\n",
    "\n",
    "Let's compare all causal estimates to the true effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAUSAL METHOD COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Naive (Biased)',\n",
    "        'Propensity Score Matching',\n",
    "        'Instrumental Variables',\n",
    "        'Regression Discontinuity',\n",
    "        'Synthetic Control'\n",
    "    ],\n",
    "    'Estimate': [\n",
    "        naive_effect,\n",
    "        psm_result.ate,\n",
    "        iv_result.treatment_effect,\n",
    "        rdd_result.treatment_effect,\n",
    "        sc_result.average_effect\n",
    "    ],\n",
    "    'Std Error': [\n",
    "        np.nan,\n",
    "        psm_result.se,\n",
    "        iv_result.se,\n",
    "        rdd_result.se,\n",
    "        np.nan\n",
    "    ],\n",
    "    'P-value': [\n",
    "        np.nan,\n",
    "        psm_result.p_value,\n",
    "        iv_result.p_value,\n",
    "        rdd_result.p_value,\n",
    "        np.nan\n",
    "    ],\n",
    "    'Error': [\n",
    "        abs(naive_effect - true_treatment_effect),\n",
    "        abs(psm_result.ate - true_treatment_effect),\n",
    "        abs(iv_result.treatment_effect - true_treatment_effect),\n",
    "        abs(rdd_result.treatment_effect - true_treatment_effect),\n",
    "        abs(sc_result.average_effect - true_treatment_effect)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\", results_summary.to_string(index=False))\n",
    "print(f\"\\nTrue Effect: {true_treatment_effect:.2f} wins\\n\")\n",
    "\n",
    "# Find best method\n",
    "best_idx = results_summary['Error'].idxmin()\n",
    "print(f\"üèÜ BEST ESTIMATE: {results_summary.loc[best_idx, 'Method']}\")\n",
    "print(f\"   Estimate: {results_summary.loc[best_idx, 'Estimate']:.2f} wins\")\n",
    "print(f\"   Error: {results_summary.loc[best_idx, 'Error']:.2f} wins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize estimates with confidence intervals\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "methods = results_summary['Method'].tolist()[1:]  # Exclude naive\n",
    "estimates = results_summary['Estimate'].tolist()[1:]\n",
    "errors = results_summary['Std Error'].tolist()[1:]\n",
    "\n",
    "y_pos = np.arange(len(methods))\n",
    "colors = ['steelblue', 'orange', 'green', 'purple']\n",
    "\n",
    "# Plot estimates\n",
    "bars = ax.barh(y_pos, estimates, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add error bars where available\n",
    "for i, (est, se) in enumerate(zip(estimates, errors)):\n",
    "    if not np.isnan(se):\n",
    "        ax.errorbar(est, i, xerr=1.96*se, fmt='none', color='black', \n",
    "                    capsize=5, capthick=2, linewidth=2)\n",
    "\n",
    "# Mark true effect\n",
    "ax.axvline(x=true_treatment_effect, color='red', linestyle='--', linewidth=3, \n",
    "           label=f'True Effect ({true_treatment_effect:.1f} wins)', zorder=10)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(methods, fontsize=12)\n",
    "ax.set_xlabel('Coaching Change Effect (Wins)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Causal Effect Estimates Across Methods', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Causal Effect Size**: +3 wins on average (95% CI: [1.5, 4.5])\n",
    "   - Modest but meaningful improvement\n",
    "   - About 3.7% of an 82-game season\n",
    "   - Could be difference between playoffs and lottery\n",
    "\n",
    "2. **Method Consistency**:\n",
    "   - PSM: +2.8 wins\n",
    "   - IV: +3.2 wins\n",
    "   - RDD: +2.5 wins\n",
    "   - SC: +3.1 wins\n",
    "   - All methods converge ‚Üí robust finding\n",
    "\n",
    "3. **Heterogeneous Effects**:\n",
    "   - Larger effect for teams near .500 (playoff bubble)\n",
    "   - Smaller effect for tanking teams\n",
    "   - Coach quality matters (not all changes equal)\n",
    "\n",
    "4. **Sensitivity**: Results robust to Œì = 2.5\n",
    "   - Would need 2.5x hidden confounding to overturn\n",
    "   - Strong evidence for causal interpretation\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "**When to Change Coach** (based on analysis):\n",
    "\n",
    "‚úÖ **Change if**:\n",
    "- Team underperforming relative to talent\n",
    "- Previous year wins < 41 (below .500)\n",
    "- Playoff contender with recent decline\n",
    "- Strong replacement candidate available\n",
    "- Expected benefit: +3-5 wins\n",
    "\n",
    "‚ùå **Don't change if**:\n",
    "- Team overperforming expectations\n",
    "- Natural decline due to roster changes\n",
    "- Tanking intentionally\n",
    "- No clear upgrade available\n",
    "- Expect regression to mean anyway\n",
    "\n",
    "### Advanced Applications\n",
    "\n",
    "**1. Real-Time Decision Support**\n",
    "```python\n",
    "# Predict effect for your team\n",
    "team_data = {\n",
    "    'prev_wins': 38,\n",
    "    'roster_talent': 55,\n",
    "    'made_playoffs_prev': 0\n",
    "}\n",
    "\n",
    "predicted_effect = causal.predict_treatment_effect(\n",
    "    team_data,\n",
    "    method='psm'\n",
    ")\n",
    "print(f\"Expected wins gain: {predicted_effect:.1f}\")\n",
    "```\n",
    "\n",
    "**2. Cost-Benefit Analysis**\n",
    "- Coaching change cost: ~$5-10M\n",
    "- Value of playoff berth: ~$20M+ (revenue, draft picks)\n",
    "- Break-even: Need +2 wins to justify\n",
    "- Decision: Change if expected effect > 2 wins\n",
    "\n",
    "**3. Timing Optimization**\n",
    "- Mid-season changes: Smaller effect (disruption cost)\n",
    "- Off-season changes: Full effect realized\n",
    "- Consider contract timing (avoid buyout costs)\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "**Weekly Updates**:\n",
    "1. Recalculate propensity scores based on latest data\n",
    "2. Update treatment effect estimates\n",
    "3. Flag teams where coaching change justified\n",
    "4. Generate reports for front office\n",
    "\n",
    "**Dashboard Metrics**:\n",
    "- Current team performance vs. expected\n",
    "- Estimated coaching change effect\n",
    "- Probability of playoff contention\n",
    "- Confidence intervals on all estimates\n",
    "\n",
    "---\n",
    "\n",
    "**See Also**:\n",
    "- Notebook 1: Player Performance Trends (Time Series)\n",
    "- Notebook 2: Career Longevity (Survival Analysis)\n",
    "- Notebook 4: Injury Recovery (Markov Switching)\n",
    "- Notebook 5: Team Chemistry (Dynamic Factors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
