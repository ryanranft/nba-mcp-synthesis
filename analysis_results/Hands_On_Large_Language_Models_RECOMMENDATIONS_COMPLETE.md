# üìö Recursive Analysis: Hands On Large Language Models

**Analysis Date:** 2025-10-21T20:09:21.519899
**Total Iterations:** 15
**Convergence Status:** ‚ùå NOT ACHIEVED
**Convergence Threshold:** 3 consecutive "Nice-to-Have only" iterations

---

## üìä Summary Statistics

| Metric | Value |
|--------|-------|
| Total Recommendations | 300 |
| Critical | 150 |
| Important | 150 |
| Nice-to-Have | 0 |
| Iterations | 15 |

---

## üîÑ Iteration Details

### Iteration 1

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 2

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 3

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 4

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 5

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 6

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 7

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 8

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 9

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 10

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 11

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 12

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 13

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 14

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

### Iteration 15

**Critical:** 10
**Important:** 10
**Nice-to-Have:** 0

#### üî¥ Critical

- {'title': 'Implement Subword Tokenization with BPE or WordPiece', 'description': 'Use subword tokenization to handle out-of-vocabulary words and improve representation of player names and basketball terms.', 'technical_details': 'Implement BPE or WordPiece tokenization using Hugging Face Tokenizers. Vocabulary size should be tuned based on dataset size. Special tokens should include beginning/end of sequence, padding, and unknown tokens.', 'implementation_steps': ['Step 1: Choose BPE or WordPiece.', 'Step 2: Train the tokenizer on a corpus of NBA articles, player bios, game reports.', 'Step 3: Integrate the tokenizer into the data preprocessing pipeline.', 'Step 4: Evaluate tokenizer performance using perplexity and coverage metrics.'], 'expected_impact': 'Improved handling of rare player names and basketball jargon, leading to better model accuracy.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Data Processing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Token Embeddings as Input to Language Models', 'description': 'Use the tokenizer to convert the raw text into tokens and feed the embedding vectors into the Large Language Model. The output is then passed through the language model to generate contextual embeddings.', 'technical_details': 'Use the embeddings outputted from the tokenizer and pass it to DeBERTaV3 or other high performing LLM', 'implementation_steps': ['Step 1: Ensure tokenizer is integrated with model input layer.', 'Step 2: Verify proper data flow and embedding vector shapes.', "Step 3: Validate model's ability to produce appropriate embeddings given known good data."], 'expected_impact': 'Enable better handling of context', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': ['Implement Subword Tokenization'], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Parallel Token Processing and KV Cache', 'description': 'Cache previously computed key and value pairs for already processed tokens for efficiency.', 'technical_details': 'Use `use_cache=True` option in the `model.generate()` to avoid redundant calculations. Ensure the GPU and memory is powerful enough to handle KV cache.', 'implementation_steps': ['Step 1: Implement check to see if caching is supported by the LLM.', 'Step 2: Store KV cache with associated tokens in a fast-access memory space.', 'Step 3: Adjust prompt pipeline to consider precomputed data when needed and remove unneeded work.', 'Step 4: Monitor performance under different numbers of concurrent users.'], 'expected_impact': 'Significant speedup in text generation, making the NBA analytics platform more responsive.', 'priority': 'CRITICAL', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Sentence Transformers for Supervised Classification', 'description': 'Leverage Sentence Transformers to create embeddings of NBA player performance reviews, and then train a logistic regression model on top of those embeddings to predict positive or negative sentiment.', 'technical_details': 'Use SentenceTransformer library to create embeddings. Train LogisticRegression classifier using scikit-learn.', 'implementation_steps': ['Step 1: Load a pre-trained Sentence Transformer model (e.g., all-mpnet-base-v2).', 'Step 2: Encode NBA player performance reviews into embeddings.', 'Step 3: Train a logistic regression model using the generated embeddings and sentiment labels.', 'Step 4: Evaluate performance (F1 score, precision, recall) using a held-out test set.'], 'expected_impact': 'Efficiently classify sentiment of NBA player performance reviews.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Fine-Tune Generative Models with Human Preferences', 'description': 'Improve an LLM by ranking outputs with preference data. Can greatly influence a language model', 'technical_details': 'The core process is having a group of people rank generated results to help the model improve. Use Reinforcement Learning to train the models', 'implementation_steps': ['Step 1: Collect preference data', 'Step 2: Train reward model', 'Step 3: Use the reward model to fine-tune LLM', 'Step 4: Reiterate on models to train them better'], 'expected_impact': "Will greatly affect an LLM's overall usefulness", 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 12. Fine-Tuning Generation Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Improve Outputs with Step-by-Step Thinking', 'description': 'Give language models the ability to take each aspect of a problem in steps, rather than as a whole to improve their overall performance and accuracy.', 'technical_details': 'Design a process to break problems into pieces. Make sure all edge cases are handled correctly.', 'implementation_steps': ['Step 1: Figure out how to break problems into steps', 'Step 2: Design individual steps', 'Step 3: Train the language model to use this structure'], 'expected_impact': 'Enables language models to solve problems better', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Add Context to Chatbot', 'description': 'Give the language model more context to make sure the bot gives the best answer. Useful in a variety of situations.', 'technical_details': 'Design the prompt to include as much context as possible. Do not sacrifice readability with longer descriptions', 'implementation_steps': ['Step 1: Brainstorm the type of context needed', 'Step 2: Add the context into prompts', 'Step 3: Evaluate the results.'], 'expected_impact': 'Much better LLM conversations', 'priority': 'CRITICAL', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement a Two-Pass Process to Improve Search Quality', 'description': 'A way to incorporate language models is through two passes. First, the system will get a number of results. Then, the system will then reorder the results based on relevance to the search.', 'technical_details': 'Develop a pipeline and reorder the responses. Implement a method to verify reordered values to ensure accuracy of the pipeline.', 'implementation_steps': ['Step 1: Make sure the pipeline works.', 'Step 2: Develop a method to reorder the responses with the LLM', 'Step 3: Report on the results of both types of searches'], 'expected_impact': 'Higher-quality and better search results for less common questions.', 'priority': 'CRITICAL', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Increase Information Availability', 'description': 'Use an LLM to add external information. This way, if external resources or tools have important information, then they can be easily accessed. Using semantic search, this system would allow information to be easily available for LLM to use.', 'technical_details': 'Develop a process to give access to the LLM to external resources. LLM should ask follow up questions when appropriate', 'implementation_steps': ['Step 1: Set up external components', 'Step 2: Connect to the LLM with a proper method and format', 'Step 3: Evaluate the performance of having this model connect to other resources'], 'expected_impact': 'Enables LLMs to use information that it might not know of.', 'priority': 'CRITICAL', 'time_estimate': '80 hours', 'dependencies': ['Add context to chatbot', 'Use LLMs', 'Have an organized way to store information, such as a Vector Database.'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Several Chains', 'description': 'An LLM is simply a string of commands. Use additional components to allow for additional improvements.', 'technical_details': 'Use memory and prompt techniques in sequential order.', 'implementation_steps': ['Step 1: Develop a prompt or a series of code using separate prompts', 'Step 2: Chain the individual pieces of code together to have more power'], 'expected_impact': 'Improved modularity in the program.', 'priority': 'CRITICAL', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

#### üü° Important

- {'title': 'Experiment with Temperature and Top_p Sampling', 'description': 'Optimize the diversity and relevance of generated text by experimenting with temperature and top_p sampling during token selection.', 'technical_details': 'Implement a configuration panel for LLM endpoint allowing temperature to be adjusted. The application should persist and report the config used for each session.', 'implementation_steps': ['Step 1: Add a web UI to control sampling config for the LLM.', 'Step 2: Track temperature and top_p setting along with all predictions.', 'Step 3: Test different settings under different scenarios and report performance metrics.'], 'expected_impact': 'Balancing diversity and relevance in generated text for different use cases in NBA analytics.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 3. Looking Inside Large Language Models', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Implement Zero-Shot Classification with Cosine Similarity', 'description': 'Employ cosine similarity to perform zero-shot classification of NBA game highlights without training data.', 'technical_details': "Use pre-trained Sentence Transformer model to create embeddings for highlight descriptions and class labels ('positive play,' 'negative play'). Classify based on cosine similarity.", 'implementation_steps': ['Step 1: Define descriptive class labels for NBA game highlights.', 'Step 2: Encode highlight descriptions and class labels using Sentence Transformer.', 'Step 3: Assign class based on highest cosine similarity score.', 'Step 4: Evaluate performance using human judgment or existing labeled data.'], 'expected_impact': 'Classify NBA game highlights without labeled training data.', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Implement Sentence Transformers for Supervised Classification'], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Flan-T5 for Sentiment Analysis', 'description': 'Use a pre-trained Flan-T5 model to analyze sentiment in NBA fan comments. Can be used in conjunction with the music preferences model.', 'technical_details': 'Utilize the Transformers library to implement Flan-T5 sentiment analysis. Need to format prompts properly for input into Flan-T5.', 'implementation_steps': ['Step 1: Load a pre-trained Flan-T5 model.', 'Step 2: Preprocess NBA fan comments and construct prompts.', 'Step 3: Generate sentiment labels using Flan-T5.', 'Step 4: Evaluate performance against a benchmark or manual labeling.'], 'expected_impact': 'Automate sentiment analysis of NBA fan comments.', 'priority': 'IMPORTANT', 'time_estimate': '20 hours', 'dependencies': [], 'source_chapter': 'Chapter 4. Text Classification', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Employ TF-IDF as a Baseline for Text Clustering', 'description': 'Leverage TF-IDF, instead of more complex language models, for a bag-of-words representation of text. Can improve performance in many different applications.', 'technical_details': 'Use TF-IDF to preprocess the model, and then add additional components', 'implementation_steps': ['Step 1: Prepare text', 'Step 2: Load TF-IDF preprocessor', 'Step 3: Evaluate the TF-IDF results', 'Step 4: Assess and improve where needed'], 'expected_impact': 'Can improve performance when a fast and cheap solution is necessary', 'priority': 'IMPORTANT', 'time_estimate': '4 hours', 'dependencies': [], 'source_chapter': 'Chapter 5. Text Clustering and Topic Modeling', 'category': 'ML', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Use Test Cases to Help Validate Outputs', 'description': 'LLMs can sometimes output incorrect text. Creating a number of test cases can increase the quality of the LLM', 'technical_details': 'Develop a method for creating and storing test cases, such as a database.', 'implementation_steps': ['Step 1: Prepare code to store the test cases', 'Step 2: Develop the test cases', 'Step 3: Add the test cases', 'Step 4: Analyze results'], 'expected_impact': 'Improves quality of output', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': [], 'source_chapter': 'Chapter 6. Prompt Engineering', 'category': 'Testing', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Utilize Hybrid Searches', 'description': 'A lot of the time, keyword searches are helpful to get an exact match for what the user is looking for. It would help to implement the ability to do hybrid searches and see which results are more valuable to the user.', 'technical_details': 'Add keyword searches in addition to LLM', 'implementation_steps': ['Step 1: Incorporate keyword matching to identify search results', 'Step 2: Incorporate an LLM to identify search results', 'Step 3: Set up both queries to function together', 'Step 4: Assess and measure the performance and improve results'], 'expected_impact': 'Addresses different use cases for both LLM and traditional searches', 'priority': 'IMPORTANT', 'time_estimate': '16 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Combine Retrieval-Augmented Generation (RAG) and the LLM', 'description': "There needs to be a process for the LLM to cite the original source, since LLMs do not necessarily generate ground-truth context and may output incorrect text. Also helpful for the system's and model's intellectual property.", 'technical_details': 'Design the system in a way where data can be easily found to be attributed to its author.', 'implementation_steps': ['Step 1: Look into a database of previous data. Create a way to store who created what, and link a created text to its sources.', 'Step 2: When LLMs write, make sure to call these data and attribute them'], 'expected_impact': 'The system would now have the ability to credit data creators', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': ['Use LLMs', 'Set test cases to help validate outputs'], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Make a Robust Architecture', 'description': "If we don't already have multiple systems to search from, then the system needs to search from new sources too, which would be a similar method to giving the LLMs outside sources.", 'technical_details': 'The structure to perform two searches simultaneously or one search first and one second.', 'implementation_steps': ['Step 1: Create all search connections', 'Step 2: Design the code to incorporate both'], 'expected_impact': 'Improves the ability to find information', 'priority': 'IMPORTANT', 'time_estimate': '40 hours', 'dependencies': [], 'source_chapter': 'Chapter 8. Semantic Search and Retrieval-Augmented Generation', 'category': 'Architecture', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Develop Special Tokenizers', 'description': 'Build a tokenizer more focused on code and whitespace so the system can better understand the nuance of programming.', 'technical_details': 'The most important thing would be making sure the tokenization properly represents code, while not ignoring context.', 'implementation_steps': ['Step 1: Pick a solid tokenizer base and build onto that.', 'Step 2: Generate new tokens and check for potential vulnerabilities.', 'Step 3: Add tokens into the model.'], 'expected_impact': 'Improves the performance of the model with code generation tasks', 'priority': 'IMPORTANT', 'time_estimate': '24 hours', 'dependencies': [], 'source_chapter': 'Chapter 2. Tokens and Embeddings', 'category': 'Performance', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}
- {'title': 'Enhance the System by Using External APIs', 'description': 'To empower the system, it is best to allow them to access external services or APIs.', 'technical_details': 'Design different endpoints that do not interrupt security. ', 'implementation_steps': ['Step 1: Implement safeguards and permissions to make sure external APIs are used safely and appropriately.', 'Step 2: Make code in the correct and accurate format and add these APIs. Try to test the data, and monitor to see how the code may break things.'], 'expected_impact': 'Better access to different pieces of information. LLMs do not know everything, and this could greatly improve the quality', 'priority': 'IMPORTANT', 'time_estimate': '80 hours', 'dependencies': [], 'source_chapter': 'Chapter 7. Advanced Text Generation Techniques and Tools', 'category': 'Security', '_source': 'gemini', '_consensus': {'sources': ['gemini'], 'count': 1, 'both_agree': False}}

---

## ‚ö†Ô∏è Convergence Not Achieved

Maximum iterations reached without achieving convergence.
Consider extending max_iterations or reviewing analysis criteria.

---

## üìù Next Steps

1. Review all recommendations
2. Prioritize Critical items
3. Create implementation plans for Important items
4. Consider Nice-to-Have items for future iterations

---

**Generated:** 2025-10-21T20:11:32.088629
**Book:** Hands On Large Language Models
**S3 Path:** books/Hands-On_Large_Language_Models.pdf
