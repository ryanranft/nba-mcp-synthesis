#!/usr/bin/env python3
"""
Test Script for Phase 7.3: Smart Context Analysis

This script tests the Smart Context Analysis functionality including:
- User context analysis
- Behavior pattern recognition
- Contextual recommendations
- Session management
- Intelligent insight generation

Author: NBA MCP Server Team
Phase: 7.3 - Smart Context Analysis
"""

import sys
import os
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from mcp_server.tools.smart_context_analysis import (
    SmartContextAnalysisEngine,
    analyze_user_context_intelligently,
    analyze_user_behavior_patterns,
    generate_contextual_recommendations,
    manage_session_context,
    generate_intelligent_insights,
    ContextInsight,
    BehaviorPattern,
    ContextualRecommendation,
    SessionContext,
)


class Phase73TestSuite:
    """Test suite for Phase 7.3 Smart Context Analysis"""

    def __init__(self):
        """Initialize the test suite"""
        self.engine = SmartContextAnalysisEngine()
        self.test_results = []
        self.start_time = time.time()

        print("=" * 60)
        print("PHASE 7.3: SMART CONTEXT ANALYSIS - TEST SUITE")
        print("=" * 60)
        print(f"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()

    def run_all_tests(self):
        """Run all test cases"""
        test_methods = [
            self.test_user_context_analysis,
            self.test_behavior_pattern_analysis,
            self.test_contextual_recommendations,
            self.test_session_context_management,
            self.test_intelligent_insights,
            self.test_context_depth_levels,
            self.test_personalization_levels,
            self.test_error_handling,
            self.test_integration_with_sports_formulas,
            self.test_performance_benchmarks,
            self.test_standalone_functions,
        ]

        for test_method in test_methods:
            try:
                print(f"Running {test_method.__name__}...")
                test_method()
                print(f"âœ“ {test_method.__name__} passed")
                self.test_results.append(
                    {"test": test_method.__name__, "status": "passed"}
                )
            except Exception as e:
                print(f"âŒ {test_method.__name__} failed: {e}")
                self.test_results.append(
                    {"test": test_method.__name__, "status": "failed", "error": str(e)}
                )
            print()

        self.print_summary()

    def test_user_context_analysis(self):
        """Test user context analysis functionality"""
        print("Testing user context analysis...")

        # Test basic context analysis
        result = self.engine.analyze_user_context_intelligently(
            user_query="Compare LeBron James and Michael Jordan's PER",
            session_history=[
                {"query": "What is PER?", "timestamp": "2024-01-01T10:00:00"},
                {"query": "Show me LeBron's stats", "timestamp": "2024-01-01T10:05:00"},
            ],
            available_data={
                "players": ["LeBron James", "Michael Jordan"],
                "metrics": ["PER"],
            },
            expertise_level="intermediate",
            context_depth="moderate",
        )

        assert result["status"] == "success", "Context analysis should succeed"
        assert "context_analysis" in result, "Should include context analysis"
        assert (
            "query_intent" in result["context_analysis"]
        ), "Should include query intent"
        assert (
            result["context_analysis"]["query_intent"]["primary_intent"]
            == "comparative"
        ), "Should detect comparative intent"

        # Test deep context analysis
        deep_result = self.engine.analyze_user_context_intelligently(
            user_query="Analyze the correlation between usage rate and team success",
            expertise_level="expert",
            context_depth="deep",
        )

        assert deep_result["status"] == "success", "Deep analysis should succeed"
        assert (
            len(deep_result["context_analysis"]["insights"]) > 0
        ), "Deep analysis should generate insights"

        print("  âœ“ Basic context analysis")
        print("  âœ“ Deep context analysis")
        print("  âœ“ Query intent detection")
        print("  âœ“ Context element extraction")

    def test_behavior_pattern_analysis(self):
        """Test user behavior pattern analysis"""
        print("Testing behavior pattern analysis...")

        # Test behavior analysis
        result = self.engine.analyze_user_behavior_patterns(
            user_id="test_user_123",
            time_period="session",
            behavior_types=["formula_usage", "query_patterns"],
            include_patterns=True,
            include_predictions=True,
            privacy_level="basic",
        )

        assert result["status"] == "success", "Behavior analysis should succeed"
        assert "behavior_analysis" in result, "Should include behavior analysis"
        assert "patterns" in result["behavior_analysis"], "Should include patterns"
        assert (
            "predictions" in result["behavior_analysis"]
        ), "Should include predictions"

        # Test different privacy levels
        detailed_result = self.engine.analyze_user_behavior_patterns(
            user_id="test_user_456", privacy_level="detailed"
        )

        assert (
            detailed_result["status"] == "success"
        ), "Detailed analysis should succeed"

        print("  âœ“ Behavior pattern identification")
        print("  âœ“ Privacy level compliance")
        print("  âœ“ Prediction generation")
        print("  âœ“ User profile updates")

    def test_contextual_recommendations(self):
        """Test contextual recommendation generation"""
        print("Testing contextual recommendations...")

        # Create mock context analysis
        context_analysis = {
            "query_intent": {
                "primary_intent": "comparative",
                "domain": "efficiency",
                "confidence": 0.8,
            },
            "user_profile": {
                "expertise_level": "intermediate",
                "preferred_formulas": ["PER", "TS%"],
            },
            "insights": [
                {
                    "insight_id": "comparative_intent",
                    "insight_type": "pattern",
                    "title": "Comparative Analysis Intent",
                    "description": "User is seeking comparative analysis",
                    "confidence": 0.8,
                    "actionable": True,
                }
            ],
        }

        # Test recommendation generation
        result = self.engine.generate_contextual_recommendations(
            context_analysis=context_analysis,
            recommendation_count=3,
            recommendation_types=["formula"],
            personalization_level="advanced",
            include_alternatives=True,
            explanation_depth="detailed",
        )

        assert result["status"] == "success", "Recommendation generation should succeed"
        assert "recommendations" in result, "Should include recommendations"
        assert len(result["recommendations"]) > 0, "Should generate recommendations"
        assert "alternatives" in result, "Should include alternatives"

        # Test different personalization levels
        basic_result = self.engine.generate_contextual_recommendations(
            context_analysis=context_analysis, personalization_level="basic"
        )

        assert (
            basic_result["status"] == "success"
        ), "Basic personalization should succeed"

        print("  âœ“ Recommendation generation")
        print("  âœ“ Personalization levels")
        print("  âœ“ Alternative recommendations")
        print("  âœ“ Explanation depth")

    def test_session_context_management(self):
        """Test session context management"""
        print("Testing session context management...")

        session_id = "test_session_789"
        context_data = {
            "current_formulas": ["PER", "TS%"],
            "analysis_goals": ["compare players"],
            "progress": 0.5,
        }

        # Test storing context
        store_result = self.engine.manage_session_context(
            session_id=session_id,
            context_data=context_data,
            context_type="analysis_state",
            operation="store",
            expiration_time=3600,
        )

        assert store_result["status"] == "success", "Context storage should succeed"
        assert (
            store_result["operation_result"]["operation"] == "store"
        ), "Should store context"

        # Test retrieving context
        retrieve_result = self.engine.manage_session_context(
            session_id=session_id, context_data={}, operation="retrieve"
        )

        assert (
            retrieve_result["status"] == "success"
        ), "Context retrieval should succeed"
        assert (
            retrieve_result["operation_result"]["operation"] == "retrieve"
        ), "Should retrieve context"

        # Test updating context
        update_data = {"progress": 0.8}
        update_result = self.engine.manage_session_context(
            session_id=session_id, context_data=update_data, operation="update"
        )

        assert update_result["status"] == "success", "Context update should succeed"

        # Test clearing context
        clear_result = self.engine.manage_session_context(
            session_id=session_id, context_data={}, operation="clear"
        )

        assert clear_result["status"] == "success", "Context clearing should succeed"

        print("  âœ“ Context storage")
        print("  âœ“ Context retrieval")
        print("  âœ“ Context updating")
        print("  âœ“ Context clearing")
        print("  âœ“ Expiration handling")

    def test_intelligent_insights(self):
        """Test intelligent insight generation"""
        print("Testing intelligent insights...")

        # Create mock analysis context
        analysis_context = {
            "formulas_used": ["PER", "TS%", "Usage Rate"],
            "results": {"player_a_per": 25.5, "player_b_per": 23.2},
            "data_points": [25.5, 23.2, 22.8, 24.1],
            "metrics": {"PER": [25.5, 23.2], "TS%": [0.58, 0.61]},
        }

        # Test insight generation
        result = self.engine.generate_intelligent_insights(
            analysis_context=analysis_context,
            insight_types=["pattern", "trend"],
            insight_depth="moderate",
            include_visualizations=True,
            include_actionable_recommendations=True,
            confidence_threshold=0.6,
            max_insights=5,
        )

        assert result["status"] == "success", "Insight generation should succeed"
        assert "insights" in result, "Should include insights"
        assert len(result["insights"]) > 0, "Should generate insights"
        assert "visualizations" in result, "Should include visualizations"
        assert (
            "actionable_recommendations" in result
        ), "Should include actionable recommendations"

        # Test different insight depths
        deep_result = self.engine.generate_intelligent_insights(
            analysis_context=analysis_context, insight_depth="deep", max_insights=10
        )

        assert (
            deep_result["status"] == "success"
        ), "Deep insight generation should succeed"

        print("  âœ“ Insight generation")
        print("  âœ“ Insight depth levels")
        print("  âœ“ Visualization suggestions")
        print("  âœ“ Actionable recommendations")
        print("  âœ“ Confidence thresholds")

    def test_context_depth_levels(self):
        """Test different context analysis depth levels"""
        print("Testing context depth levels...")

        query = "Analyze player efficiency metrics"

        # Test shallow depth
        shallow_result = self.engine.analyze_user_context_intelligently(
            user_query=query, context_depth="shallow"
        )

        assert shallow_result["status"] == "success", "Shallow analysis should succeed"

        # Test moderate depth
        moderate_result = self.engine.analyze_user_context_intelligently(
            user_query=query, context_depth="moderate"
        )

        assert (
            moderate_result["status"] == "success"
        ), "Moderate analysis should succeed"
        assert len(moderate_result["context_analysis"]["insights"]) >= len(
            shallow_result["context_analysis"]["insights"]
        ), "Moderate should have more insights"

        # Test deep depth
        deep_result = self.engine.analyze_user_context_intelligently(
            user_query=query, context_depth="deep"
        )

        assert deep_result["status"] == "success", "Deep analysis should succeed"
        assert len(deep_result["context_analysis"]["insights"]) >= len(
            moderate_result["context_analysis"]["insights"]
        ), "Deep should have more insights"

        print("  âœ“ Shallow depth analysis")
        print("  âœ“ Moderate depth analysis")
        print("  âœ“ Deep depth analysis")
        print("  âœ“ Depth progression")

    def test_personalization_levels(self):
        """Test different personalization levels"""
        print("Testing personalization levels...")

        context_analysis = {
            "query_intent": {"primary_intent": "comparative", "domain": "efficiency"},
            "user_profile": {"expertise_level": "intermediate"},
            "insights": [],
        }

        # Test no personalization
        none_result = self.engine.generate_contextual_recommendations(
            context_analysis=context_analysis, personalization_level="none"
        )

        assert none_result["status"] == "success", "No personalization should succeed"

        # Test basic personalization
        basic_result = self.engine.generate_contextual_recommendations(
            context_analysis=context_analysis, personalization_level="basic"
        )

        assert (
            basic_result["status"] == "success"
        ), "Basic personalization should succeed"

        # Test advanced personalization
        advanced_result = self.engine.generate_contextual_recommendations(
            context_analysis=context_analysis, personalization_level="advanced"
        )

        assert (
            advanced_result["status"] == "success"
        ), "Advanced personalization should succeed"

        # Test full personalization
        full_result = self.engine.generate_contextual_recommendations(
            context_analysis=context_analysis, personalization_level="full"
        )

        assert full_result["status"] == "success", "Full personalization should succeed"

        print("  âœ“ No personalization")
        print("  âœ“ Basic personalization")
        print("  âœ“ Advanced personalization")
        print("  âœ“ Full personalization")

    def test_error_handling(self):
        """Test error handling and edge cases"""
        print("Testing error handling...")

        # Test invalid session context operation
        invalid_result = self.engine.manage_session_context(
            session_id="nonexistent_session", context_data={}, operation="retrieve"
        )

        assert (
            invalid_result["status"] == "error"
        ), "Should handle invalid session gracefully"

        # Test empty context analysis
        empty_result = self.engine.generate_contextual_recommendations(
            context_analysis={}, recommendation_count=5
        )

        assert (
            empty_result["status"] == "success"
        ), "Should handle empty context analysis gracefully"
        assert (
            empty_result["metadata"]["fallback_mode"] == True
        ), "Should use fallback mode for empty context"

        # Test invalid insight types
        invalid_insights_result = self.engine.generate_intelligent_insights(
            analysis_context={"test": "data"}, insight_types=["invalid_type"]
        )

        # This should still succeed but with limited insights
        assert (
            invalid_insights_result["status"] == "success"
        ), "Should handle invalid insight types gracefully"

        print("  âœ“ Invalid session handling")
        print("  âœ“ Empty context handling")
        print("  âœ“ Invalid insight types")
        print("  âœ“ Graceful error recovery")

    def test_integration_with_sports_formulas(self):
        """Test integration with sports analytics formulas"""
        print("Testing sports formula integration...")

        # Test context analysis with sports-specific queries
        sports_result = self.engine.analyze_user_context_intelligently(
            user_query="Calculate PER for LeBron James with 25 points, 8 rebounds, 8 assists",
            available_data={
                "player": "LeBron James",
                "stats": {"points": 25, "rebounds": 8, "assists": 8},
            },
            expertise_level="advanced",
        )

        assert (
            sports_result["status"] == "success"
        ), "Sports context analysis should succeed"
        assert (
            sports_result["context_analysis"]["query_intent"]["domain"] == "efficiency"
        ), "Should identify efficiency domain"

        # Test behavior analysis with formula usage
        behavior_result = self.engine.analyze_user_behavior_patterns(
            user_id="sports_analyst", behavior_types=["formula_usage"]
        )

        assert (
            behavior_result["status"] == "success"
        ), "Sports behavior analysis should succeed"

        # Test recommendations with sports context
        sports_context = {
            "query_intent": {"primary_intent": "comparative", "domain": "efficiency"},
            "user_profile": {
                "expertise_level": "advanced",
                "preferred_formulas": ["PER", "TS%", "Usage Rate"],
            },
            "insights": [],
        }

        sports_rec_result = self.engine.generate_contextual_recommendations(
            context_analysis=sports_context, recommendation_types=["formula"]
        )

        assert (
            sports_rec_result["status"] == "success"
        ), "Sports recommendations should succeed"
        assert (
            len(sports_rec_result["recommendations"]) > 0
        ), "Should generate sports recommendations"

        print("  âœ“ Sports query analysis")
        print("  âœ“ Formula usage patterns")
        print("  âœ“ Sports-specific recommendations")
        print("  âœ“ Domain identification")

    def test_performance_benchmarks(self):
        """Test performance benchmarks for key functions"""
        print("Testing performance benchmarks...")

        # Benchmark context analysis
        start_time = time.time()
        for _ in range(10):
            self.engine.analyze_user_context_intelligently(
                user_query="Test query for performance", context_depth="moderate"
            )
        context_time = time.time() - start_time
        print(f"  Context Analysis: {context_time:.2f}s (10 iterations)")

        # Benchmark behavior analysis
        start_time = time.time()
        for _ in range(5):
            self.engine.analyze_user_behavior_patterns(
                user_id=f"perf_user_{_}", behavior_types=["formula_usage"]
            )
        behavior_time = time.time() - start_time
        print(f"  Behavior Analysis: {behavior_time:.2f}s (5 iterations)")

        # Benchmark recommendation generation
        context_analysis = {
            "query_intent": {"primary_intent": "comparative", "domain": "efficiency"},
            "user_profile": {"expertise_level": "intermediate"},
            "insights": [],
        }

        start_time = time.time()
        for _ in range(10):
            self.engine.generate_contextual_recommendations(
                context_analysis=context_analysis, recommendation_count=3
            )
        rec_time = time.time() - start_time
        print(f"  Recommendation Generation: {rec_time:.2f}s (10 iterations)")

        # Benchmark session management
        start_time = time.time()
        for i in range(20):
            self.engine.manage_session_context(
                session_id=f"perf_session_{i}",
                context_data={"test": "data"},
                operation="store",
            )
        session_time = time.time() - start_time
        print(f"  Session Management: {session_time:.2f}s (20 operations)")

        # Benchmark insight generation
        analysis_context = {
            "formulas_used": ["PER", "TS%"],
            "results": {"per": 25.5},
            "metrics": {"PER": [25.5, 23.2]},
        }

        start_time = time.time()
        for _ in range(5):
            self.engine.generate_intelligent_insights(
                analysis_context=analysis_context, insight_types=["pattern", "trend"]
            )
        insight_time = time.time() - start_time
        print(f"  Insight Generation: {insight_time:.2f}s (5 iterations)")

        total_time = (
            context_time + behavior_time + rec_time + session_time + insight_time
        )
        print(f"  Total Benchmark Time: {total_time:.2f}s")

        # Performance assertions
        assert context_time < 5.0, "Context analysis should be fast"
        assert behavior_time < 3.0, "Behavior analysis should be fast"
        assert rec_time < 3.0, "Recommendation generation should be fast"
        assert session_time < 2.0, "Session management should be fast"
        assert insight_time < 2.0, "Insight generation should be fast"

    def test_standalone_functions(self):
        """Test standalone function implementations"""
        print("Testing standalone functions...")

        # Test standalone context analysis
        standalone_context = analyze_user_context_intelligently(
            user_query="Test standalone context analysis",
            expertise_level="intermediate",
        )

        assert (
            standalone_context["status"] == "success"
        ), "Standalone context analysis should succeed"

        # Test standalone behavior analysis
        standalone_behavior = analyze_user_behavior_patterns(
            user_id="standalone_user", behavior_types=["formula_usage"]
        )

        assert (
            standalone_behavior["status"] == "success"
        ), "Standalone behavior analysis should succeed"

        # Test standalone recommendations
        context_analysis = {
            "query_intent": {"primary_intent": "comparative", "domain": "efficiency"},
            "user_profile": {"expertise_level": "intermediate"},
            "insights": [],
        }

        standalone_rec = generate_contextual_recommendations(
            context_analysis=context_analysis, recommendation_count=3
        )

        assert (
            standalone_rec["status"] == "success"
        ), "Standalone recommendations should succeed"

        # Test standalone session management
        standalone_session = manage_session_context(
            session_id="standalone_session",
            context_data={"test": "standalone"},
            operation="store",
        )

        assert (
            standalone_session["status"] == "success"
        ), "Standalone session management should succeed"

        # Test standalone insights
        analysis_context = {
            "formulas_used": ["PER"],
            "results": {"per": 25.5},
            "metrics": {"PER": [25.5]},
        }

        standalone_insights = generate_intelligent_insights(
            analysis_context=analysis_context, insight_types=["pattern"]
        )

        assert (
            standalone_insights["status"] == "success"
        ), "Standalone insights should succeed"

        print("  âœ“ Standalone context analysis")
        print("  âœ“ Standalone behavior analysis")
        print("  âœ“ Standalone recommendations")
        print("  âœ“ Standalone session management")
        print("  âœ“ Standalone insights")

    def print_summary(self):
        """Print test summary"""
        end_time = time.time()
        total_time = end_time - self.start_time

        passed_tests = [r for r in self.test_results if r["status"] == "passed"]
        failed_tests = [r for r in self.test_results if r["status"] == "failed"]

        print("=" * 60)
        print("PHASE 7.3: SMART CONTEXT ANALYSIS - TEST SUMMARY")
        print("=" * 60)
        print(f"Total Tests: {len(self.test_results)}")
        print(f"Passed: {len(passed_tests)}")
        print(f"Failed: {len(failed_tests)}")
        print(f"Success Rate: {len(passed_tests)/len(self.test_results)*100:.1f}%")
        print(f"Total Time: {total_time:.2f} seconds")
        print()

        if failed_tests:
            print("FAILED TESTS:")
            for test in failed_tests:
                print(f"  âŒ {test['test']}: {test.get('error', 'Unknown error')}")
            print()

        if len(passed_tests) == len(self.test_results):
            print("ðŸŽ‰ ALL TESTS PASSED! Phase 7.3 implementation is working correctly.")
        else:
            print(
                f"âš ï¸  {len(failed_tests)} test(s) failed. Please review the implementation."
            )

        print("=" * 60)


def main():
    """Main test execution function"""
    try:
        test_suite = Phase73TestSuite()
        test_suite.run_all_tests()

        # Return exit code based on test results
        failed_tests = [r for r in test_suite.test_results if r["status"] == "failed"]
        return 0 if len(failed_tests) == 0 else 1

    except Exception as e:
        print(f"Test suite failed with error: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
