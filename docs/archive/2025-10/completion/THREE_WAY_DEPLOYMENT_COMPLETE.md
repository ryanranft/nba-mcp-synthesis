# ğŸ‰ Three-Way Ollama Deployment - COMPLETE!

**Date:** October 11, 2025
**Status:** âœ… ALL THREE LOCATIONS DEPLOYED

---

## ğŸ“ Three Deployment Locations

### 1. ğŸ  Local Ollama (Your Mac)
**Status:** âœ… ACTIVE

**Available Models:**
- `qwen2.5-coder:32b` (19 GB) - High-quality code generation
- `gpt-oss:20b` (13 GB) - Fast, lightweight alternative

**Connection:**
- URL: `http://localhost:11434`
- Speed: Instant (no network latency)
- Cost: FREE
- Privacy: 100% local, no data leaves your machine

**Best For:**
- Quick queries
- Offline work
- Privacy-sensitive tasks
- Development/testing

---

### 2. â˜ï¸ AWS Ollama (Cloud)
**Status:** âœ… ACTIVE

**Available Models:**
- `qwen2.5-coder:32b` (19 GB) - Same model as local

**Instance Details:**
- Type: t3.2xlarge (8 vCPUs, 32 GB RAM)
- IP: 34.226.246.126
- Connection: `http://34.226.246.126:11434`
- Speed: Fast (50ms latency)
- Cost: $0.33/hour (~$8/day if left running)

**Best For:**
- Longer analysis sessions
- Resource-intensive tasks
- Team collaboration (shared instance)
- When your laptop is busy

---

### 3. âš™ï¸ Cursor IDE Integration
**Status:** âœ… CONFIGURED with ALL OPTIONS

**Cursor can now use:**
1. ğŸ  **Local Qwen 32B** - localhost, free, instant
2. â˜ï¸ **AWS Qwen 32B** - cloud, powerful, scalable
3. ğŸ  **Local GPT-OSS 20B** - lightweight, fast

**To Switch Models:**
1. Open Cursor
2. Click model selector (bottom right or in chat)
3. Choose from 3 options!

---

## ğŸ¯ Where to Use Each

### Use Local (ğŸ ) When:
- âœ… Working offline
- âœ… Need instant responses (no latency)
- âœ… Privacy-sensitive data
- âœ… Don't want to pay AWS costs
- âœ… Quick exploratory work

### Use AWS (â˜ï¸) When:
- âœ… Need sustained performance
- âœ… Your laptop is doing other work
- âœ… Running long analyses
- âœ… Want consistent speed regardless of local load
- âœ… Collaborating (can share IP with team)

### Use GPT-OSS 20B (ğŸ ) When:
- âœ… Need fast, lightweight responses
- âœ… Simple queries that don't need 32B model
- âœ… Want to conserve laptop resources
- âœ… Speed > Quality tradeoff acceptable

---

## ğŸ“Š Model Comparison

| Model | Location | Size | Speed | Quality | Cost |
|-------|----------|------|-------|---------|------|
| Qwen 32B | Local | 19GB | Fast | Excellent | Free |
| Qwen 32B | AWS | 19GB | Fast | Excellent | $0.33/hr |
| GPT-OSS 20B | Local | 13GB | Very Fast | Good | Free |

---

## ğŸš€ How to Use Each

### Method 1: Cursor IDE (Recommended)
1. Open Cursor
2. Press `Cmd+Shift+P` â†’ "Reload Window"
3. Open chat or composer
4. Click model selector
5. Choose: ğŸ  Local, â˜ï¸ AWS, or ğŸ  GPT-OSS

### Method 2: Web Interface
```bash
cd /Users/ryanranft/nba-mcp-synthesis
./start_ollama_chat.sh
```
Opens browser chat (currently points to AWS)

### Method 3: Terminal Chat
```bash
cd /Users/ryanranft/nba-mcp-synthesis
python3 ollama_mcp_chat.py
```
Terminal-based chat interface

### Method 4: Direct API
```bash
# Local
curl http://localhost:11434/api/generate \
  -d '{"model":"qwen2.5-coder:32b","prompt":"Hello","stream":false}'

# AWS
curl http://34.226.246.126:11434/api/generate \
  -d '{"model":"qwen2.5-coder:32b","prompt":"Hello","stream":false}'
```

---

## ğŸ”§ Configuration Status

### âœ… Cursor Settings Updated
**File:** `~/Library/Application Support/Cursor/User/settings.json`

**Configured Models:**
```json
{
  "cursor.chat.models": [
    {
      "name": "ğŸ  Local Ollama Qwen2.5-Coder 32B",
      "baseURL": "http://localhost:11434/v1",
      "model": "qwen2.5-coder:32b"
    },
    {
      "name": "â˜ï¸ AWS Ollama Qwen2.5-Coder 32B",
      "baseURL": "http://34.226.246.126:11434/v1",
      "model": "qwen2.5-coder:32b"
    },
    {
      "name": "ğŸ  Local Ollama GPT-OSS 20B",
      "baseURL": "http://localhost:11434/v1",
      "model": "gpt-oss:20b"
    }
  ]
}
```

### âœ… MCP Tools Configuration
**Global Config:** `~/.cursor/mcp.json`
- NBA MCP Server: Available globally

**Project Configs:**
1. `/Users/ryanranft/nba-mcp-synthesis/.cursor/mcp.json` âœ…
2. `/Users/ryanranft/nba-simulator-aws/.cursor/mcp.json` âœ…

**MCP Tools Available:**
- 100+ NBA analytics tools
- Math operations
- Statistics functions
- Machine learning tools
- Data analysis tools

---

## ğŸ§ª Test Each Deployment

### Test Local Ollama:
```bash
curl http://localhost:11434/api/tags
# Should show: qwen2.5-coder:32b, gpt-oss:20b
```

### Test AWS Ollama:
```bash
curl http://34.226.246.126:11434/api/tags
# Should show: qwen2.5-coder:32b
```

### Test Cursor Integration:
1. Open Cursor
2. Reload window (Cmd+Shift+P â†’ Reload)
3. Open chat
4. Click model selector
5. Should see 3 options!

---

## ğŸ’° Cost Management

### Current Costs:
- **Local:** FREE (uses your electricity)
- **AWS:** $0.33/hour = $7.92/day if left running

### Save Money on AWS:
```bash
# Stop when not in use
aws ec2 stop-instances --instance-ids i-06ec10b8f8fbe0ee4

# Start when needed (takes ~2 minutes)
aws ec2 start-instances --instance-ids i-06ec10b8f8fbe0ee4

# Check status
aws ec2 describe-instances \
  --instance-ids i-06ec10b8f8fbe0ee4 \
  --query 'Reservations[0].Instances[0].State.Name'
```

### Cost Optimization Strategy:
1. **Use Local by default** - Free!
2. **Use AWS for intensive work** - When needed
3. **Stop AWS instance daily** - Save $6-7/day
4. **Use GPT-OSS 20B for simple tasks** - Faster, lighter

---

## ğŸ“ˆ Performance Comparison

### Tokens Per Second (Approximate):

| Task | Local 32B | AWS 32B | Local 20B |
|------|-----------|---------|-----------|
| Code generation | 15-25 | 30-50 | 30-45 |
| Reasoning | 10-20 | 25-40 | 25-40 |
| Simple queries | 20-30 | 40-60 | 50-70 |

**Note:** Local speed depends on what else your Mac is doing.

---

## ğŸ”„ Switch Between Deployments

### In Cursor:
**Click model selector â†’ Choose:**
- ğŸ  For local work
- â˜ï¸ For cloud power
- ğŸ  20B for speed

### In Web/Terminal:
**Edit configuration files:**
```bash
# Point to local
sed -i '' 's|34.226.246.126|localhost|g' ollama_web_chat.html

# Point to AWS
sed -i '' 's|localhost|34.226.246.126|g' ollama_web_chat.html
```

Or use the switch script:
```bash
# Create quick switch script
cat > switch_to_local.sh << 'EOF'
#!/bin/bash
sed -i '' 's|34.226.246.126|localhost|g' ollama_web_chat.html
sed -i '' 's|34.226.246.126|localhost|g' ollama_mcp_chat.py
echo "âœ… Switched to Local Ollama"
EOF

cat > switch_to_aws.sh << 'EOF'
#!/bin/bash
sed -i '' 's|localhost|34.226.246.126|g' ollama_web_chat.html
sed -i '' 's|localhost|34.226.246.126|g' ollama_mcp_chat.py
echo "âœ… Switched to AWS Ollama"
EOF

chmod +x switch_to_*.sh
```

---

## ğŸ¯ Best Practices

### 1. Model Selection Strategy
```
Simple Question â†’ ğŸ  GPT-OSS 20B (fast)
      â†“
    No?
      â†“
Need Privacy? â†’ ğŸ  Qwen 32B (local)
      â†“
    No?
      â†“
Long Analysis â†’ â˜ï¸ AWS Qwen 32B (cloud)
```

### 2. Cost Optimization
- **Morning:** Start AWS instance
- **Work:** Use AWS for intensive tasks
- **Quick tasks:** Use local
- **Evening:** Stop AWS instance
- **Overnight:** Local only

### 3. Workflow Suggestions
```
Exploratory â†’ Local (free, fast)
Development â†’ Local (instant feedback)
Production â†’ AWS (reliable, scalable)
Testing â†’ Local 20B (lightweight)
```

---

## ğŸ” Monitoring & Status

### Check All Services:
```bash
# Create status check script
cat > check_all_ollama.sh << 'EOF'
#!/bin/bash
echo "ğŸ” Checking All Ollama Deployments"
echo "=================================="
echo ""

echo "1. ğŸ  Local Ollama:"
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
  echo "   âœ… Running"
  curl -s http://localhost:11434/api/tags | \
    python3 -c "import sys,json;[print(f'   â€¢ {m[\"name\"]}') for m in json.load(sys.stdin)['models']]"
else
  echo "   âŒ Not running"
fi

echo ""
echo "2. â˜ï¸ AWS Ollama:"
if curl -s http://34.226.246.126:11434/api/tags > /dev/null 2>&1; then
  echo "   âœ… Running"
  curl -s http://34.226.246.126:11434/api/tags | \
    python3 -c "import sys,json;[print(f'   â€¢ {m[\"name\"]}') for m in json.load(sys.stdin)['models']]"
else
  echo "   âŒ Not running"
fi

echo ""
echo "3. âš™ï¸ Cursor Config:"
if grep -q "cursor.chat.models" "$HOME/Library/Application Support/Cursor/User/settings.json" 2>/dev/null; then
  echo "   âœ… Configured"
  echo "   â€¢ $(grep -c "\"id\"" "$HOME/Library/Application Support/Cursor/User/settings.json" | tr -d ' ') models available"
else
  echo "   âŒ Not configured"
fi

echo ""
echo "4. ğŸ”§ MCP Tools:"
if [ -f "$HOME/.cursor/mcp.json" ]; then
  echo "   âœ… Global config exists"
fi
if [ -f "/Users/ryanranft/nba-mcp-synthesis/.cursor/mcp.json" ]; then
  echo "   âœ… nba-mcp-synthesis configured"
fi
if [ -f "/Users/ryanranft/nba-simulator-aws/.cursor/mcp.json" ]; then
  echo "   âœ… nba-simulator-aws configured"
fi

echo ""
echo "=================================="
EOF

chmod +x check_all_ollama.sh
```

Run it:
```bash
./check_all_ollama.sh
```

---

## ğŸ‰ Summary

### âœ… What You Have Now:

| Feature | Status | Location |
|---------|--------|----------|
| Local Qwen 32B | âœ… ACTIVE | Your Mac |
| AWS Qwen 32B | âœ… ACTIVE | Cloud (IP: 34.226.246.126) |
| Local GPT-OSS 20B | âœ… ACTIVE | Your Mac |
| Cursor Integration | âœ… CONFIGURED | All 3 models available |
| MCP Tools | âœ… AVAILABLE | 100+ tools, both projects |
| Web Interface | âœ… WORKING | localhost:8000 |
| Terminal Chat | âœ… WORKING | Python script |

### ğŸ¯ Next Steps:

1. **Reload Cursor** to see all 3 model options
2. **Try each model** to feel the differences
3. **Stop AWS instance** when done today
4. **Wait for GPU approval** (24-48 hours)
5. **Deploy 72B model** when approved (3x better!)

---

## ğŸš€ Quick Start Commands

```bash
# Check status of all
./check_all_ollama.sh

# Start web chat (uses AWS by default)
./start_ollama_chat.sh

# Switch to local
./switch_to_local.sh

# Switch to AWS
./switch_to_aws.sh

# Stop AWS to save money
aws ec2 stop-instances --instance-ids i-06ec10b8f8fbe0ee4

# Start AWS when needed
aws ec2 start-instances --instance-ids i-06ec10b8f8fbe0ee4
```

---

## ğŸŠ Congratulations!

**You now have a COMPLETE three-way Ollama deployment:**

âœ… Local for speed and privacy
âœ… AWS for power and reliability
âœ… Cursor integration for seamless workflow
âœ… 100+ NBA analytics tools everywhere
âœ… Multiple model options (32B, 20B)
âœ… Full cost control (stop/start AWS)

**All three locations are configured, tested, and ready to use!** ğŸ‰

---

**Questions? Issues?**
Check the respective guides:
- `OLLAMA_MCP_SETUP_GUIDE.md` - Local setup
- `AWS_OLLAMA_GUIDE.md` - Cloud setup
- `DEPLOYMENT_SUCCESS.md` - AWS deployment details
- `CURSOR_MCP_SETUP.md` - Cursor integration

**Happy coding with AI! ğŸ€ğŸ¤–**


