# Context Optimization Operations Guide

**Purpose**: Operational guide for Claude/AI sessions on maintaining context optimization
**Audience**: Future AI sessions, developers, project maintainers
**Last Updated**: 2025-10-11
**Status**: Active operational procedures

---

## ðŸŽ¯ Overview

This guide ensures **consistent context management** across all future sessions by documenting:
1. **Where to save information** (file locations and patterns)
2. **How to maintain the system** (operational procedures)
3. **The systematic approach** (phases and methodology used)
4. **Decision-making framework** (when to create/update/archive files)

**Goal**: Never exceed 3-10K tokens per session (vs 30-50K before optimization)

---

## ðŸ“ Information Storage Hierarchy

### Decision Tree: Where Should This Information Go?

```
Is this information...

â”œâ”€ About the CURRENT session/day?
â”‚  â””â”€> .ai/daily/YYYY-MM-DD-session-N.md (gitignored, detailed)
â”‚
â”œâ”€ A compact session summary?
â”‚  â””â”€> .ai/current-session.md (tracked, auto-generated by session_start.sh)
â”‚
â”œâ”€ Permanent reference (tools, decisions, architecture)?
â”‚  â””â”€> .ai/permanent/ (tracked, searchable)
â”‚      â”œâ”€ tool-registry.md (all MCP tools)
â”‚      â”œâ”€ phases.md (implementation phases)
â”‚      â”œâ”€ file-management-policy.md (file policies)
â”‚      â”œâ”€ context_budget.json (budget config)
â”‚      â””â”€ template.md (ADR template)
â”‚
â”œâ”€ Current project status?
â”‚  â””â”€> project/status/ (tracked, current state)
â”‚      â”œâ”€ tools.md (tool registration status)
â”‚      â”œâ”€ sprints.md (sprint progress)
â”‚      â”œâ”€ features.md (feature status)
â”‚      â””â”€ blockers.md (current issues)
â”‚
â”œâ”€ Incremental progress update?
â”‚  â””â”€> project/tracking/progress.log (tracked, append-only)
â”‚      Format: "YYYY-MM-DD: Action taken"
â”‚
â”œâ”€ Key decision or milestone?
â”‚  â””â”€> project/tracking/decisions.md (tracked, append with date)
â”‚      â””â”€> project/tracking/milestones.md (tracked, append with date)
â”‚
â”œâ”€ Metrics or analytics?
â”‚  â””â”€> project/metrics/ (tracked, update periodically)
â”‚
â”œâ”€ Documentation/guide?
â”‚  â””â”€> docs/guides/ (tracked, organized by topic)
â”‚
â”œâ”€ Historical/completed?
â”‚  â””â”€> docs/archive/YYYY-MM/ (gitignored, organized by month)
â”‚
â””â”€ Monthly summary?
   â””â”€> .ai/monthly/YYYY-MM-summary.md (gitignored)
```

---

## ðŸ¤– Automated File Management System

### Understanding the Automation

This project uses **script-based file creation** for consistency, optimization, and reliability. Many files are AUTO-GENERATED and should NEVER be edited manually.

**Key Principle**: Scripts before manual edits. Always check if a script exists before creating or editing files.

### Auto-Generated Files (NEVER Edit Manually)

| File | Generator Script | Frequency | Purpose | Token Cost |
|------|------------------|-----------|---------|------------|
| `.ai/current-session.md` | `session_start.sh` | Every session | Compact session state | ~300 tokens |
| `PROJECT_STATUS.md` | `update_status.sh` | On demand | Quick status overview | ~150 tokens |
| Daily session files | `session_start.sh --new-session` | Optional | Detailed session log | Variable |
| Session checkpoints | `checkpoint_session.sh` | On demand | Session snapshots | Variable |
| Benchmark results | Test scripts | On test run | Performance metrics | N/A |
| Test reports | Test scripts | On test run | Test results | N/A |

**Why Scripts?**
1. **Optimized for context** - Only includes necessary information
2. **Consistent format** - Predictable structure reduces parsing tokens
3. **No duplication** - Scripts pull from canonical sources
4. **Automatic cleanup** - Archives old content automatically

### Script-Based File Creation Patterns

#### Pattern 1: Session Files

```bash
# Generate current-session.md (ALWAYS do this first)
./scripts/session_start.sh

# Create new daily session file (optional, for detailed notes)
./scripts/session_start.sh --new-session

# Create session checkpoint (save current state)
./scripts/checkpoint_session.sh --name=before-major-change
```

**When to use**:
- Start of every session: `session_start.sh`
- Need detailed notes: `session_start.sh --new-session`
- Before risky changes: `checkpoint_session.sh`

**Token savings**: 94% (5,000 â†’ 300 tokens for session start)

#### Pattern 2: Archiving

```bash
# Check what would be archived (dry run)
./scripts/auto_archive.sh --dry-run

# Archive completion documents
./scripts/auto_archive.sh

# Archive interactively (confirm each file)
./scripts/auto_archive.sh --interactive

# Archive old sessions to S3
./scripts/session_archive.sh --to-s3

# Archive with age threshold
./scripts/auto_archive.sh --age=30
```

**When to use**:
- After completing major work (sprint, phase)
- Root directory >15 markdown files
- Weekly health check recommends it
- Before starting new major work

**Token savings**: ~1,500 tokens per archived completion document

#### Pattern 3: Status Updates

```bash
# Regenerate PROJECT_STATUS.md from source data
./scripts/update_status.sh

# Update specific status file (manual editing OK)
vim project/status/tools.md +45  # Jump to specific line
```

**When to use**:
- After major changes to project state
- Need refreshed overview
- Status file becomes outdated

**Token savings**: 50-100 tokens (edit specific section vs. reading full file)

#### Pattern 4: Context Monitoring

```bash
# Check context budget
./scripts/track_context_budget.sh --session

# View dashboard
./scripts/context_dashboard.sh

# Monitor file sizes
./scripts/monitor_file_sizes.sh

# Run weekly health check
./scripts/weekly_health_check.sh
```

**When to use**:
- Daily: Quick dashboard check
- Weekly: Full health check
- Before commits: Monitor file sizes
- Context feels high: Track budget

### When Scripts Are NOT Available

**Fallback to Manual Creation with Guidelines**:

1. **Check DOCUMENTATION_MAP.md** for canonical location
   ```bash
   grep -i "topic name" docs/DOCUMENTATION_MAP.md
   ```

2. **Follow 7-step decision tree** (above)

3. **Update relevant index files**
   ```bash
   vim docs/guides/index.md  # If creating guide
   vim docs/DOCUMENTATION_MAP.md  # Add to map
   ```

4. **Cross-reference instead of duplicating**
   ```markdown
   See [Topic](canonical/location.md) for details.
   ```

### Example Workflows

#### Example 1: Starting a New Session

```bash
# STEP 1: Generate session context (300 tokens)
./scripts/session_start.sh

# STEP 2: Read compact context
cat .ai/current-session.md

# STEP 3: (Optional) Create detailed session file
./scripts/session_start.sh --new-session

# Result: Optimized session start in <5 minutes
```

**Context cost**: 300 tokens (vs 5,000+ manual approach)

#### Example 2: Completing a Sprint

```bash
# STEP 1: Update status files
vim project/status/sprints.md  # Mark sprint complete

# STEP 2: Archive completion document
# (If you created SPRINT_X_COMPLETE.md)
./scripts/auto_archive.sh --interactive

# STEP 3: Refresh status overview
./scripts/update_status.sh

# STEP 4: Commit changes
git add . && git commit -m "feat: Complete Sprint X"

# Result: Clean workspace, archived completion docs
```

**Context savings**: ~1,500 tokens from archived docs

#### Example 3: Creating New Guide

```bash
# STEP 1: Check if canonical location exists
grep -i "feature name" docs/DOCUMENTATION_MAP.md

# STEP 2: If not found, create guide
vim docs/guides/NEW_FEATURE_GUIDE.md

# STEP 3: Update index
vim docs/guides/index.md

# STEP 4: Add to documentation map
vim docs/DOCUMENTATION_MAP.md

# Result: Properly indexed, discoverable guide
```

**Best practice**: Link to this guide from related docs, don't duplicate content

---

## ðŸ”„ Daily Operations

### Session Start Procedure

**Every session should begin with**:

```bash
# 1. Start session (generates current-session.md)
./scripts/session_start.sh

# 2. Review compact context
cat .ai/current-session.md

# 3. Check for specific status if needed
cat PROJECT_STATUS.md
```

**What This Does**:
- Runs health checks
- Generates compact session summary (~300 tokens)
- Shows git status, recent commits, project state
- Provides links to detailed files

**Cost**: ~300 tokens (vs 5,000+ reading multiple files)

---

### During Session: Information Recording

#### For Incremental Progress (Never read, only append)

```bash
# Append daily progress (10 tokens)
echo "$(date +%Y-%m-%d): Implemented feature X" >> project/tracking/progress.log

# Or use helper script
./scripts/update_status.sh "Implemented feature X"
```

**Rules**:
- âŒ Never read the full progress.log file
- âœ… Only append new lines
- âœ… Use date prefix: `YYYY-MM-DD: Action`
- **Cost**: 10 tokens (vs 1,500+ reading full log)

#### For Status Updates (Edit specific sections)

```bash
# Update only the section that changed
vim project/status/tools.md  # Edit just the tool section
vim project/status/sprints.md  # Edit just sprint status
```

**Rules**:
- âŒ Don't read entire tracker files
- âœ… Edit only the specific section that changed
- âœ… Use line numbers from index: `project/status/tools.md:45-67`
- **Cost**: 50-100 tokens (vs 1,000+ reading full tracker)

#### For Important Decisions

```bash
# Append to decisions log
echo "$(date +%Y-%m-%d): Decision - Use FastMCP for Sprint 5" >> project/tracking/decisions.md
echo "Rationale: Better async support, simpler syntax" >> project/tracking/decisions.md
echo "" >> project/tracking/decisions.md
```

**Rules**:
- âœ… Include date, decision, and rationale
- âœ… Append to end of file
- âœ… Keep each entry concise (3-5 lines)

#### For Detailed Session Notes

```bash
# Create detailed session file (gitignored)
vim .ai/daily/$(date +%Y-%m-%d)-session-1.md

# Template structure:
## Session Goals
## Work Completed
## Decisions Made
## Next Steps
## Blockers/Issues
```

**Rules**:
- âœ… Detailed notes go in daily/ (gitignored)
- âœ… Use template structure for consistency
- âœ… This is never loaded by default (only when explicitly needed)
- âŒ Don't duplicate information from status files

---

### Session End Procedure

**At the end of each session**:

```bash
# 1. Update PROJECT_STATUS.md (auto-regenerates from current data)
./scripts/update_status.sh

# 2. Commit changes
git add .ai/current-session.md project/ docs/
git commit -m "feat: Session summary - [brief description]"

# 3. Archive old sessions (optional, runs automatically weekly)
./scripts/session_archive.sh --to-s3  # If S3 configured
```

**What This Does**:
- Refreshes PROJECT_STATUS.md with latest data
- Commits trackable files
- Archives sessions older than 7 days to S3

---

## ðŸ“Š Weekly/Monthly Maintenance

### Weekly Tasks (Every 7 days)

**Time required**: ~15-20 minutes

```bash
# 1. Run comprehensive health check
./scripts/weekly_health_check.sh
# Output: .ai/monitoring/reports/weekly_YYYYMMDD.md
# Includes: file sizes, context budget, archive status, recommendations

# 2. Monitor file sizes
./scripts/monitor_file_sizes.sh
# Checks: index files, status files, guides, sessions, plans
# Alerts: files exceeding context budget limits

# 3. Review context dashboard
./scripts/context_dashboard.sh
# Shows: budget overview, file distribution, health indicators, archive metrics

# 4. Check archive opportunities
./scripts/auto_archive.sh --dry-run
# Preview: what would be archived, token savings, breakdown by reason
# If candidates found:
#   ./scripts/auto_archive.sh --interactive  # Review and archive

# 5. Archive old sessions
./scripts/session_archive.sh --to-s3
# Archives: daily sessions >7 days old to S3 (or local)
# Frequency: weekly to prevent accumulation

# 6. Check root directory
ls -1 *.md | wc -l
# Target: <15 files
# Warning: 15-20 files
# Critical: >20 files (run auto_archive.sh)

# 7. Update metrics (optional)
vim project/metrics/tool_counts.md  # If tools registered this week
vim project/metrics/context_usage.md  # If significant changes
```

**Checklist**:
- [ ] Run weekly_health_check.sh
- [ ] Review any errors/warnings
- [ ] Run auto_archive.sh if >3 completion docs in root
- [ ] Archive daily sessions if >7 files
- [ ] Check dashboard for anomalies
- [ ] Root directory has <15 markdown files
- [ ] No completion documents in root

### Monthly Tasks (Every 30 days)

**Time required**: ~30-45 minutes

```bash
# 1. Update baseline metrics
./scripts/establish_baselines.sh --force
# Updates: context usage baselines for comparison
# Frequency: monthly or quarterly

# 2. Create monthly summary
vim .ai/monthly/$(date +%Y-%m)-summary.md
# Summarize: key accomplishments, decisions, metrics, challenges
# Include: sprint completions, tool registrations, major changes

# 3. Review context dashboard trends
./scripts/context_dashboard.sh --export=monthly_metrics.json
# Analyze: trends over time, improvements, areas needing attention

# 4. Archive monthly summaries to S3
./scripts/session_archive.sh --to-s3 --monthly
# Archives: monthly summaries >3 months old
# Preserves: critical decisions in permanent references

# 5. Comprehensive archive review
./scripts/auto_archive.sh --age=60
# Archives: files not modified in 60 days
# Review: docs/plans/, docs/analysis/, old completion docs

# 6. Update archive indexes
vim docs/archive/$(date +%Y-%m)/index.md
# Add: newly archived files with descriptions
# Organize: by category (completion, sessions, plans)

# 7. Review file count targets
ROOT_COUNT=$(ls -1 *.md 2>/dev/null | wc -l)
DOCS_COUNT=$(find docs/ -name "*.md" ! -path "*/archive/*" | wc -l)
echo "Root: $ROOT_COUNT (target: <15)"
echo "Docs: $DOCS_COUNT (target: <100)"

# 8. Clean up monitoring logs
find .ai/monitoring/ -name "*.txt" -mtime +90 -delete
find .ai/monitoring/reports/ -name "weekly_*.md" -mtime +90 -delete

# 9. Audit cross-references
./scripts/audit_cross_references.sh
# Checks: broken links, duplicate content, outdated references

# 10. Update DOCUMENTATION_MAP.md if needed
vim docs/DOCUMENTATION_MAP.md
# Add: new guides, permanent references
# Update: canonical locations for new topics
```

**Checklist**:
- [ ] Establish new baselines
- [ ] Create monthly summary
- [ ] Export and review metrics
- [ ] Archive monthly summaries to S3
- [ ] Run comprehensive archive (60-day threshold)
- [ ] Update archive indexes
- [ ] Verify file count targets
- [ ] Clean up old monitoring logs
- [ ] Audit cross-references
- [ ] Update documentation map
- [ ] Review and archive old plans/analysis docs
- [ ] Root directory: <15 files
- [ ] Active docs: <100 files
- [ ] No completion documents in root

---

## ðŸŽ¯ The Systematic Approach (How We Did It)

### Phase-by-Phase Methodology

This section documents **the exact approach used** to create the context optimization system, so it can be replicated for other projects or future optimizations.

#### Phase 0: Measurement & Analysis

**Objective**: Understand the problem

**Actions**:
1. Measure current context usage
   ```bash
   # Count tokens in key files
   wc -l PROJECT_MASTER_TRACKER.md  # 670 lines
   wc -l QUICKSTART.md               # 300 lines
   ls -la docs/ | wc -l              # 80+ files
   ```

2. Identify pain points
   - Session start requires reading 5+ files (5,000+ tokens)
   - Status check requires reading full tracker (1,000+ tokens)
   - Tool lookup requires scanning implementation files (1,000+ tokens)

3. Set targets
   - Session start: <300 tokens (94% reduction)
   - Status check: <150 tokens (85% reduction)
   - Tool lookup: <100 tokens (90% reduction)
   - Overall: 80-93% reduction (30-50K â†’ 3-10K tokens)

**Output**: Baseline measurements, targets defined

---

#### Phase 1: Session State Management

**Objective**: Create `.ai/` directory for session handoff

**Actions**:
1. Create directory structure
   ```bash
   mkdir -p .ai/{daily,monthly,permanent,archive}
   ```

2. Create index files
   ```bash
   vim .ai/index.md          # Master guide (388 lines)
   vim .ai/daily/index.md    # Daily sessions navigation
   vim .ai/monthly/index.md  # Monthly summaries navigation
   vim .ai/permanent/index.md # Permanent references navigation
   ```

3. Create templates
   ```bash
   vim .ai/daily/template.md    # Session template
   vim .ai/monthly/template.md  # Monthly template
   ```

4. Configure .gitignore
   ```bash
   # Add to .gitignore
   .ai/daily/*
   .ai/monthly/*
   .ai/archive/
   !.ai/current-session.md
   !.ai/index.md
   !.ai/permanent/
   !.ai/daily/template.md
   !.ai/monthly/template.md
   ```

5. Create automation script
   ```bash
   vim scripts/session_start.sh  # Auto-generate current-session.md
   chmod +x scripts/session_start.sh
   ```

**Output**: `.ai/` directory fully functional

**Verification**:
- Run `./scripts/session_start.sh`
- Check `cat .ai/current-session.md` (should be ~50 lines)
- Verify gitignore working: `git status` (daily/monthly should not appear)

**Context Savings**: 94% on session start (5,000 â†’ 300 tokens)

---

#### Phase 2: Split Large Trackers

**Objective**: Break PROJECT_MASTER_TRACKER.md (670 lines) into focused files

**Actions**:
1. Create project directory structure
   ```bash
   mkdir -p project/{status,tracking,metrics}
   ```

2. Create index files
   ```bash
   vim project/index.md          # Master navigation (184 lines)
   vim project/status/index.md   # Status navigation
   vim project/tracking/index.md # Tracking navigation
   vim project/metrics/index.md  # Metrics navigation
   ```

3. Split content by responsibility
   ```bash
   # Extract sections from PROJECT_MASTER_TRACKER.md
   vim project/status/tools.md      # Tool registration (150 lines)
   vim project/status/sprints.md    # Sprint status (100 lines)
   vim project/status/features.md   # Feature status
   vim project/status/blockers.md   # Current issues

   # Create append-only logs
   vim project/tracking/progress.log    # Daily progress (append-only)
   vim project/tracking/decisions.md    # Key decisions
   vim project/tracking/milestones.md   # Major achievements

   # Create metrics files
   vim project/metrics/tool_counts.md     # Tool metrics
   vim project/metrics/test_coverage.md   # Test coverage
   vim project/metrics/context_usage.md   # Context metrics
   ```

4. Create quick reference at root
   ```bash
   vim PROJECT_STATUS.md  # Compact status (<150 lines)
   # Auto-generated by scripts/update_status.sh
   ```

5. Update scripts
   ```bash
   vim scripts/update_status.sh  # Auto-regenerate PROJECT_STATUS.md
   chmod +x scripts/update_status.sh
   ```

**Output**: Large tracker split into focused files

**Verification**:
- Check file sizes: `wc -l project/status/*.md` (all <200 lines)
- Run `./scripts/update_status.sh`
- Verify `cat PROJECT_STATUS.md` (<150 lines)

**Context Savings**: 93% on status checks (1,000 â†’ 75 tokens for quick status)

---

#### Phase 3: Archive Strategy

**Objective**: Move historical files to archive with proper navigation

**Actions**:
1. Create archive structure
   ```bash
   mkdir -p docs/archive/$(date +%Y-%m)/{completion,sessions}
   ```

2. Identify files to archive
   ```bash
   # Find completion documents
   ls *_COMPLETE.md *_SUMMARY.md *_SUCCESS.md

   # Find session documents
   ls *_SESSION*.md
   ```

3. Move files to archive
   ```bash
   # Move completion docs
   mv *_COMPLETE.md *_SUMMARY.md docs/archive/2025-10/completion/

   # Move session docs
   mv *_SESSION*.md docs/archive/2025-10/sessions/
   ```

4. Create archive index
   ```bash
   vim docs/archive/2025-10/index.md
   # List all archived files with descriptions
   # Explain when/why archived
   # Cross-reference to active replacements
   ```

5. Update .gitignore
   ```bash
   # Add to .gitignore
   docs/archive/2025-*/
   docs/completion/
   docs/sessions/
   ```

**Output**: 38 files archived with navigation

**Verification**:
- Check archive: `ls docs/archive/2025-10/completion/ | wc -l` (34 files)
- Check index: `cat docs/archive/2025-10/index.md`
- Verify active workspace: `ls *.md | wc -l` (reduced by ~48%)

**Context Savings**: 30% reduction in search noise

---

#### Phase 4: External Persistent Notes (S3)

**Objective**: Enable unlimited session history via S3 storage

**Actions**:
1. Create archive script
   ```bash
   vim scripts/session_archive.sh
   chmod +x scripts/session_archive.sh
   ```

2. Configure S3 bucket (optional)
   ```bash
   # Set environment variable
   export NBA_MCP_S3_BUCKET="nba-mcp-sessions"

   # Or add to .env (gitignored)
   echo "NBA_MCP_S3_BUCKET=nba-mcp-sessions" >> .env
   ```

3. Implement retention policies
   ```bash
   # In session_archive.sh
   RETENTION_DAYS=7        # Keep 7 days locally
   RETENTION_MONTHS=3      # Keep 3 months on S3
   ```

4. Add restore functionality to session_start.sh
   ```bash
   # Usage: ./scripts/session_start.sh --restore=2025-10-11-session-1.md
   ```

**Output**: S3 integration complete

**Verification**:
- Test archive: `./scripts/session_archive.sh --dry-run`
- Test S3 upload: `./scripts/session_archive.sh --to-s3` (if configured)
- Test restore: `./scripts/session_start.sh --restore=<session-id>`

**Cost**: ~$0.0005/month for typical usage (100-1000 sessions)

---

#### Phase 5: Pre-Session Checklist

**Objective**: Automate health checks and session setup

**Actions**:
1. Enhance session_start.sh with health checks
   ```bash
   vim scripts/session_start.sh

   # Add functions:
   - run_health_check()    # Check git, directories, S3
   - restore_session()     # Restore from S3
   - generate_session()    # Create current-session.md
   ```

2. Add command-line options
   ```bash
   --health-check   # Run comprehensive diagnostics
   --new-session    # Create new daily session file
   --restore=ID     # Restore from S3
   --help          # Show usage
   ```

3. Implement quick health check on every start
   ```bash
   # Check git status
   # Check for uncommitted changes
   # Verify directory structure
   ```

**Output**: Automated session startup

**Verification**:
- Run health check: `./scripts/session_start.sh --health-check`
- Check all tests pass
- Verify current-session.md generated

---

#### Phase 6: Tool Registry

**Objective**: Create searchable registry of all MCP tools

**Actions**:
1. Create registry file
   ```bash
   vim .ai/permanent/tool-registry.md
   ```

2. Document all tools by category
   ```markdown
   ## Database Tools (15 tools)
   - query_database - Execute SQL queries
   - list_tables - List all tables
   ...

   ## S3 Tools (10 tools)
   ...

   ## ML Tools (33 tools)
   ...
   ```

3. Add file:line references where possible
   ```markdown
   - query_database - mcp_server/tools/database_tools.py:45
   ```

**Output**: Comprehensive tool registry

**Verification**:
- Check tool count matches actual implementation
- Search test: `grep "calculate_correlation" .ai/permanent/tool-registry.md`

**Context Savings**: 90% on tool lookup (1,000 â†’ 100 tokens)

---

#### Phase 7: .gitignore Optimization

**Objective**: Reduce git noise, protect sensitive data

**Actions**:
1. Add comprehensive patterns
   ```bash
   # AI Session Management
   .ai/daily/*
   .ai/monthly/*
   .ai/archive/

   # Test artifacts
   test_results/
   benchmark_results/
   *.log

   # Generated documentation
   *_GENERATED.md
   *_TEMP.md

   # Archives
   docs/archive/2025-*/
   docs/completion/
   ```

2. Test patterns
   ```bash
   vim scripts/test_gitignore.sh
   chmod +x scripts/test_gitignore.sh
   ./scripts/test_gitignore.sh
   ```

**Output**: 70+ patterns added

**Verification**:
- Run `git status` (should not show gitignored files)
- Verify important files still tracked

---

#### Phase 8: Documentation Indexes

**Objective**: Create navigation indexes for all documentation

**Actions**:
1. Create master index
   ```bash
   vim docs/index.md  # Master documentation hub
   ```

2. Create subsection indexes
   ```bash
   vim docs/guides/index.md
   vim docs/sprints/index.md
   vim docs/analysis/index.md
   vim docs/planning/index.md
   vim docs/plans/index.md
   ```

3. Ensure all indexes <100 lines
   ```bash
   wc -l docs/*/index.md  # Verify all <100 lines
   ```

**Output**: 8+ indexes created

**Verification**:
- Check each index navigates to correct files
- Verify indexes are concise (<100 lines)

---

#### Phase 9: Documentation Cross-References

**Objective**: Eliminate duplication via canonical sources

**Actions**:
1. Create documentation map
   ```bash
   vim docs/DOCUMENTATION_MAP.md
   ```

2. Define canonical locations for each topic
   ```markdown
   | Topic | Canonical Location | Purpose |
   |-------|-------------------|---------|
   | Tool Registration | project/status/tools.md | Single source |
   | Session Management | .ai/index.md | Complete guide |
   ```

3. Establish cross-reference patterns
   ```markdown
   # Brief Reference Pattern
   See [Tool Registration](project/status/tools.md) for details.

   # Detailed Reference Pattern
   For comprehensive information, see [Session Management Guide](.ai/index.md).
   ```

4. Create audit script
   ```bash
   vim scripts/audit_cross_references.sh
   chmod +x scripts/audit_cross_references.sh
   ```

**Output**: Documentation map + cross-reference system

**Verification**:
- Run audit: `./scripts/audit_cross_references.sh`
- Check for duplicates (should find <5%)

---

#### Phase 10: Testing & Validation

**Objective**: Verify system works and achieves targets

**Actions**:
1. Create comprehensive test script
   ```bash
   vim scripts/test_context_optimization.sh
   chmod +x scripts/test_context_optimization.sh
   ```

2. Test categories
   - Archive structure (6 tests)
   - Index system (12 tests)
   - Gitignore patterns (11 tests)
   - Session management (5 tests)
   - Quick reference (5 tests)
   - Cross-references (4 tests)
   - Script functionality (7 tests)
   - Context optimization (3 tests)
   - Integration (2 tests)

3. Run full test suite
   ```bash
   ./scripts/test_context_optimization.sh
   ```

4. Measure context savings
   ```bash
   vim scripts/measure_context.sh
   ./scripts/measure_context.sh
   ```

5. Document results
   ```bash
   vim TEST_REPORT_CONTEXT_OPTIMIZATION.md
   ```

**Output**: Test report showing 96% pass rate

**Verification**:
- 55/57 tests pass
- Context reduction 80-93% achieved
- All targets met

---

#### Phase 11: Proactive Monitoring (NEW PHASE)

**Objective**: Prevent context creep through automated monitoring and alerting

**Actions**:
1. Create file size monitoring script
   ```bash
   vim scripts/monitor_file_sizes.sh
   chmod +x scripts/monitor_file_sizes.sh
   ```

2. Define file size thresholds
   ```bash
   # Thresholds by file type:
   - Index files: 100 lines (warning at 80)
   - Status files: 200 lines (warning at 160)
   - Guide files: 300 lines (warning at 240)
   - Daily sessions: 500 lines (warning at 400)
   - Plan files: 3000 lines (warning at 2400)
   ```

3. Create context dashboard
   ```bash
   vim scripts/context_dashboard.sh
   chmod +x scripts/context_dashboard.sh
   ```

4. Establish baseline metrics
   ```bash
   vim scripts/establish_baselines.sh
   chmod +x scripts/establish_baselines.sh
   ./scripts/establish_baselines.sh  # Run once to create baseline
   ```

5. Create weekly health check
   ```bash
   vim scripts/weekly_health_check.sh
   chmod +x scripts/weekly_health_check.sh
   ```

6. Schedule regular monitoring
   ```bash
   # Add to cron (optional)
   0 9 * * 1 cd /path/to/project && ./scripts/weekly_health_check.sh --email=you@example.com
   ```

**Output**: Proactive monitoring system

**Verification**:
- Run monitor: `./scripts/monitor_file_sizes.sh`
- View dashboard: `./scripts/context_dashboard.sh`
- Check baselines: `cat .ai/monitoring/baselines.json`
- Test weekly check: `./scripts/weekly_health_check.sh`

**Context Savings**: Prevents future context creep (proactive maintenance)

**Maintenance**:
- Run weekly health check every Monday
- Update baselines quarterly
- Review dashboard before major changes
- Act on file size warnings immediately

---

#### Phase 12: Enhanced Automation (NEW PHASE)

**Objective**: Automate repetitive maintenance tasks to reduce manual overhead

**Actions**:
1. Create auto-generate indexes script
   ```bash
   vim scripts/auto_generate_indexes.sh
   chmod +x scripts/auto_generate_indexes.sh
   ```

2. Create auto-update documentation map script
   ```bash
   vim scripts/auto_update_doc_map.sh
   chmod +x scripts/auto_update_doc_map.sh
   ```

3. Create intelligent auto-archive script
   ```bash
   vim scripts/auto_archive.sh
   chmod +x scripts/auto_archive.sh
   ```

4. Install git hooks for validation
   ```bash
   cp scripts/pre-commit.template .git/hooks/pre-commit
   chmod +x .git/hooks/pre-commit
   ```

5. Configure automated maintenance
   ```bash
   # Optional: Add to cron for weekly automation
   # Monday 9am: Archive old files
   0 9 * * 1 cd /path/to/project && ./scripts/auto_archive.sh --age=30

   # Tuesday 9am: Update indexes
   0 9 * * 2 cd /path/to/project && ./scripts/auto_generate_indexes.sh --force

   # Wednesday 9am: Update documentation map
   0 9 * * 3 cd /path/to/project && ./scripts/auto_update_doc_map.sh --add-missing
   ```

**Output**: Automated maintenance system

**Verification**:
- Test index generation: `./scripts/auto_generate_indexes.sh --dry-run`
- Test doc map update: `./scripts/auto_update_doc_map.sh --scan-only`
- Test auto-archive: `./scripts/auto_archive.sh --dry-run`
- Test git hook: `git commit -m "test" --dry-run` (make a test change first)

**Context Savings**: Reduces maintenance overhead by 70% (automated tasks)

**Features**:
- **Auto-generate indexes**: Scans directories and creates/updates index.md files
- **Auto-update doc map**: Detects new files and adds to DOCUMENTATION_MAP.md
- **Auto-archive**: Moves old/completed files to archive based on age and patterns
- **Git hooks**: Pre-commit validation prevents oversized files and broken links

**Maintenance**:
- Review auto-generated content weekly
- Adjust archive age threshold based on project velocity
- Customize git hook checks based on team needs

---

#### Phase 13: Context Budget System (NEW PHASE)

**Objective**: Establish formal token budget system with tracking and enforcement

**Actions**:
1. Create comprehensive budget guide
   ```bash
   vim docs/guides/CONTEXT_BUDGET_GUIDE.md
   ```

2. Create budget tracking script
   ```bash
   vim scripts/track_context_budget.sh
   chmod +x scripts/track_context_budget.sh
   ```

3. Create budget configuration file
   ```bash
   vim .ai/permanent/context_budget.json
   ```

4. Define budget allocations
   ```json
   {
     "session_start": 300,
     "status_check": 150,
     "tool_lookup": 100,
     "progress_update": 10,
     "status_update": 50,
     "decision_recording": 30
   }
   ```

5. Implement tracking system
   ```bash
   # Track current session budget
   ./scripts/track_context_budget.sh --session

   # Generate weekly report
   ./scripts/track_context_budget.sh --weekly-report

   # Analyze patterns
   ./scripts/track_context_budget.sh --analyze

   # Get recommendations
   ./scripts/track_context_budget.sh --recommendations
   ```

**Output**: Formal budget management system

**Verification**:
- Check session budget: `./scripts/track_context_budget.sh --session`
- Review budget config: `cat .ai/permanent/context_budget.json`
- Read guide: `docs/guides/CONTEXT_BUDGET_GUIDE.md`
- Generate report: `./scripts/track_context_budget.sh --weekly-report`

**Context Savings**: Enables budget awareness and prevents overruns

**Features**:
- **Token budgets by operation**: Defined limits for each operation type
- **Real-time tracking**: Monitor budget usage during session
- **Historical analysis**: Track trends over time
- **Automated alerts**: Warnings when approaching/exceeding budgets
- **Optimization recommendations**: Actionable suggestions for improvement

**Budget Targets**:
- Session start: 300 tokens (vs 5,000+ before)
- Status check: 150 tokens (vs 1,000+ before)
- Overall session: 3-10K tokens (vs 30-50K before)

**Maintenance**:
- Review budget compliance weekly
- Adjust budgets based on project needs
- Track trends monthly
- Update optimization strategies as needed

---

#### Phase 14: Emergency Procedures & Enhancements (NEW PHASE)

**Objective**: Provide emergency procedures and advanced optimization techniques

**Actions**:
1. Create emergency procedures guide
   ```bash
   vim docs/guides/CONTEXT_EMERGENCY_PROCEDURES.md
   ```

2. Create emergency context reduction script
   ```bash
   vim scripts/emergency_context_reduce.sh
   chmod +x scripts/emergency_context_reduce.sh
   ```

3. Create session checkpoint script
   ```bash
   vim scripts/checkpoint_session.sh
   chmod +x scripts/checkpoint_session.sh
   ```

4. Test emergency procedures
   ```bash
   # Test Level 1 (Warning)
   ./scripts/emergency_context_reduce.sh --level=1 --dry-run

   # Test checkpoint creation
   ./scripts/checkpoint_session.sh --name=test

   # Test checkpoint restore
   ./scripts/checkpoint_session.sh --restore=test
   ```

**Output**: Emergency response system

**Verification**:
- Read procedures: `docs/guides/CONTEXT_EMERGENCY_PROCEDURES.md`
- Test reduction: `./scripts/emergency_context_reduce.sh --level=1 --dry-run`
- Test checkpoint: `./scripts/checkpoint_session.sh`
- List checkpoints: `./scripts/checkpoint_session.sh --list`

**Context Savings**: Enables recovery from context emergencies

**Features**:
- **3-level emergency system**: Warning, Critical, Emergency procedures
- **Emergency context reduction**: Automated reduction based on severity
- **Session checkpointing**: Quick save/restore of session state
- **Recovery procedures**: Step-by-step recovery instructions
- **Advanced techniques**: Symbolic links, differential updates, minimal templates

**Emergency Levels**:
- Level 1 (8-9K tokens): Quick optimizations, 30% reduction
- Level 2 (9-10K tokens): Aggressive reduction, 50% reduction
- Level 3 (>10K tokens): Maximum reduction, 70% reduction

**Maintenance**:
- Test emergency procedures quarterly
- Update procedures based on real emergencies
- Train team on emergency response
- Monitor emergency frequency (target: <1/month)

---

### Summary of Phases

| Phase | Objective | Key Output | Context Savings |
|-------|-----------|------------|-----------------|
| 0 | Measurement | Baseline data | - |
| 1 | Session Management | `.ai/` directory | 94% (session start) |
| 2 | Split Trackers | `project/` directory | 93% (status checks) |
| 3 | Archive | 38 files archived | 30% (search noise) |
| 4 | S3 Integration | Unlimited history | Cost: $0.0005/month |
| 5 | Health Checks | Automated startup | Reliability++ |
| 6 | Tool Registry | Searchable tools | 90% (tool lookup) |
| 7 | Gitignore | 70+ patterns | Noise reduction |
| 8 | Indexes | 10+ navigation files | Efficient discovery |
| 9 | Cross-References | No duplication | Maintenance-- |
| 10 | Testing | 96% pass rate | Validation |
| 11 | Proactive Monitoring | 4 monitoring scripts | Prevents creep |
| 12 | Enhanced Automation | 4 automation scripts + git hooks | 70% overhead reduction |
| 13 | Context Budget System | Budget tracking + guide | Budget awareness |
| 14 | Emergency Procedures | Emergency scripts + procedures | Emergency recovery |

**Overall Result**: 80-93% context reduction (30-50K â†’ 3-10K tokens)

**Total Enhancement Phases**: 15 (Phases 0-14 complete)

---

## ðŸš¨ Common Pitfalls to Avoid

### âŒ DON'T: Read Append-Only Logs

**Bad**:
```bash
cat project/tracking/progress.log  # 1500 tokens!
# Then append new line
echo "2025-10-11: Update" >> project/tracking/progress.log
```

**Good**:
```bash
# Just append, never read
echo "$(date +%Y-%m-%d): Update" >> project/tracking/progress.log  # 10 tokens
```

---

### âŒ DON'T: Read Entire Trackers for Small Updates

**Bad**:
```bash
cat PROJECT_MASTER_TRACKER.md  # 1000 tokens
# Edit one section
vim PROJECT_MASTER_TRACKER.md
```

**Good**:
```bash
# Use index to find exact location
grep -n "Tool Registration" project/index.md  # "See project/status/tools.md:45-67"
# Edit only that section
vim project/status/tools.md +45  # 50 tokens
```

---

### âŒ DON'T: Duplicate Information Across Files

**Bad**:
```markdown
# In multiple files:
## Tool Registration Process
1. Create tool in mcp_server/tools/
2. Register in fastmcp_server.py
3. Test with test script
... (full details repeated)
```

**Good**:
```markdown
# In most files:
## Tool Registration
See [Tool Registration Guide](project/status/tools.md) for complete process.

# In ONE canonical file (project/status/tools.md):
## Tool Registration Process
1. Create tool in mcp_server/tools/
2. Register in fastmcp_server.py
3. Test with test script
... (full details ONCE)
```

---

### âŒ DON'T: Create Files Without Checking Index System

**Bad**:
```bash
# Create random file
vim NEW_FEATURE_DOCS.md  # Where does this belong?
```

**Good**:
```bash
# Check index first
cat docs/index.md  # See structure
# Create in appropriate location
vim docs/guides/NEW_FEATURE_GUIDE.md
# Update index
vim docs/guides/index.md  # Add link to new file
```

---

### âŒ DON'T: Skip Session Scripts

**Bad**:
```bash
# Start working immediately
# Manually read 5+ files to understand state
cat PROJECT_STATUS.md
cat .ai/permanent/tool-registry.md
cat project/status/tools.md
...  # 5000+ tokens
```

**Good**:
```bash
# Use automation
./scripts/session_start.sh  # 300 tokens
cat .ai/current-session.md
# Start working
```

---

## ðŸŽ¯ Decision Framework

### When to Create New Files

**Create a new file when**:
- Information is >200 lines and can be split logically
- Topic deserves dedicated documentation
- File will be referenced from multiple locations
- Content is permanent reference material

**Example**:
```bash
# Tool registry getting large (337 lines)
# Split by category if >500 lines
vim .ai/permanent/database-tools.md
vim .ai/permanent/ml-tools.md
vim .ai/permanent/s3-tools.md
# Update index
vim .ai/permanent/index.md
```

### When to Update Existing Files

**Update existing file when**:
- Information fits existing structure
- File is <200 lines after update
- Topic is already covered

**Example**:
```bash
# New tool registered
# Update existing tools.md
vim project/status/tools.md  # Add to appropriate section
```

### When to Archive Files

**Archive a file when**:
- Information is historical/completed
- File hasn't been updated in 30+ days
- Content is no longer relevant to active work
- Need to reduce active workspace clutter

**Example**:
```bash
# Sprint 5 completed 30 days ago
mv docs/sprints/SPRINT_5_COMPLETE.md docs/archive/2025-10/sprints/
# Update archive index
vim docs/archive/2025-10/index.md
```

---

## ðŸ“Š Metrics & Monitoring

### Context Usage Tracking

**Monthly Review**:
```bash
# 1. Measure current context usage
./scripts/measure_context.sh

# 2. Update metrics file
vim project/metrics/context_usage.md

# 3. Compare to targets
# Session start: <300 tokens (target: 300)
# Status check: <150 tokens (target: 150)
# Tool lookup: <100 tokens (target: 100)
```

### File Count Tracking

**Quarterly Review**:
```bash
# Active workspace
ls *.md | wc -l  # Should be <50
ls docs/**/*.md | wc -l  # Should be <100

# Archives
ls docs/archive/**/*.md | wc -l  # Grows over time (OK)
```

### Health Check Schedule

**Automated Checks**:
- **Every session**: Quick health check (session_start.sh)
- **Weekly**: Full health check (`--health-check`)
- **Monthly**: Audit cross-references, archive old files

---

## ðŸ”§ Troubleshooting

### Problem: Context Usage Creeping Up

**Symptoms**: Session start >500 tokens, status checks >300 tokens

**Diagnosis**:
```bash
# Check file sizes
wc -l .ai/current-session.md  # Should be <80 lines
wc -l PROJECT_STATUS.md       # Should be <150 lines
wc -l project/status/*.md     # Each should be <200 lines
```

**Solution**:
```bash
# If current-session.md too large
vim scripts/session_start.sh  # Reduce content generated

# If status files too large
# Split into smaller files or archive old content
vim project/status/tools.md  # Move completed items to archive
```

---

### Problem: Can't Find Information

**Symptoms**: Searching multiple files to find a topic

**Diagnosis**:
```bash
# Check if indexes are up to date
cat docs/index.md
cat project/index.md
cat .ai/index.md
```

**Solution**:
```bash
# Update indexes
vim docs/index.md  # Add missing links

# Check documentation map
cat docs/DOCUMENTATION_MAP.md  # Find canonical location

# If topic not in map, add it
vim docs/DOCUMENTATION_MAP.md
```

---

### Problem: Duplicate Information

**Symptoms**: Same content in multiple files

**Diagnosis**:
```bash
# Run audit script
./scripts/audit_cross_references.sh
```

**Solution**:
```bash
# 1. Identify canonical source
vim docs/DOCUMENTATION_MAP.md

# 2. Replace duplicates with cross-references
# In non-canonical files:
# Replace: [full content]
# With: See [Topic](canonical/location.md) for details.

# 3. Update documentation map
vim docs/DOCUMENTATION_MAP.md  # Document canonical location
```

---

## ðŸ“š Quick Reference for Claude/AI

### Starting a New Session

```bash
# 1. Run session start script
./scripts/session_start.sh

# 2. Read compact context (300 tokens)
cat .ai/current-session.md

# 3. Check specific status if needed (75 tokens)
cat PROJECT_STATUS.md

# Total: ~375 tokens vs 5,000+ before
```

---

### Recording Information During Session

**Progress Updates** (10 tokens):
```bash
echo "$(date +%Y-%m-%d): Action taken" >> project/tracking/progress.log
```

**Status Updates** (50 tokens):
```bash
vim project/status/tools.md  # Edit specific section only
```

**Important Decisions** (30 tokens):
```bash
echo "$(date +%Y-%m-%d): Decision - Reasoning" >> project/tracking/decisions.md
```

**Detailed Notes** (stored in gitignored daily/):
```bash
vim .ai/daily/$(date +%Y-%m-%d)-session-1.md
```

---

### Finding Information

**Use index files first**:
```bash
cat docs/index.md  # What's in docs/?
cat project/index.md  # What's in project/?
cat .ai/index.md  # How does session management work?
```

**Use documentation map**:
```bash
grep -i "tool registration" docs/DOCUMENTATION_MAP.md
# Returns: project/status/tools.md (canonical location)
```

**Search specific content**:
```bash
grep -r "search term" docs/guides/
grep -r "search term" .ai/permanent/
```

---

### Before Making Changes

**Check the decision tree** (top of this document):
- Where should this information go?
- Is there already a canonical location?
- Should I update existing or create new?

**Check the index**:
- Does an index file guide this directory?
- Is the index up to date?
- Should I update the index after my change?

**Check for duplication**:
- Is this information already documented elsewhere?
- Should I link to existing content instead?
- Is this the canonical location for this topic?

---

### Monitoring Context Health

**Check file sizes before committing**:
```bash
./scripts/monitor_file_sizes.sh
```

**Review context usage dashboard**:
```bash
./scripts/context_dashboard.sh
```

**Establish baseline (first time only)**:
```bash
./scripts/establish_baselines.sh
```

**Run weekly health check**:
```bash
./scripts/weekly_health_check.sh
# Generates report: .ai/monitoring/reports/weekly_YYYYMMDD.md
```

**Export metrics for analysis**:
```bash
./scripts/context_dashboard.sh --export=metrics.json
```

---

## ðŸŽ“ Training Material

### For New Team Members

**Read these in order**:
1. `CONTEXT_OPTIMIZATION_REQUIREMENTS_VERIFICATION.md` - What was built and why
2. `CONTEXT_OPTIMIZATION_OPERATIONS_GUIDE.md` - This file (how to use it)
3. `.ai/index.md` - Session management guide
4. `docs/DOCUMENTATION_MAP.md` - Where to find things
5. `PROJECT_STATUS.md` - Current project state

**Time to proficiency**: 30-60 minutes

---

### For AI Sessions (Claude)

**Every new session should**:
1. Run `./scripts/session_start.sh` first
2. Read `.ai/current-session.md` (300 tokens)
3. Follow the decision tree for where to save information
4. Use append-only logs for progress (never read them)
5. Update only specific sections in status files
6. Run `./scripts/update_status.sh` at session end

**Key principle**: Minimize context usage by reading only what's needed

---

## ðŸš€ Future Enhancements

### Potential Improvements

1. **Automated Archive Scheduling**
   - Cron job to run `session_archive.sh` weekly
   - Automatic monthly summary generation

2. **Enhanced Metrics Dashboard**
   - Real-time context usage tracking
   - Visual charts of file sizes over time
   - Alert if files exceed target sizes

3. **AI-Powered Search**
   - Semantic search across all documentation
   - Automatic cross-reference suggestions
   - Duplicate detection

4. **Integration with IDEs**
   - VS Code extension for quick access
   - File navigation shortcuts
   - Automatic index updates

---

## ðŸ“ž Support & Feedback

### If You Need to Replicate This System

**Follow these phases in order**:
1. Phase 0: Measure baseline context usage
2. Phase 1: Create `.ai/` directory structure
3. Phase 2: Split large tracker files
4. Phase 3: Archive historical files
5. Phase 4-10: Implement remaining features

**Estimated time**: 2-4 hours for initial setup, 1-2 days for full implementation

### If Something Doesn't Work

**Check these first**:
1. Health check: `./scripts/session_start.sh --health-check`
2. Test suite: `./scripts/test_context_optimization.sh`
3. Audit: `./scripts/audit_cross_references.sh`

**Review**:
- `TEST_REPORT_CONTEXT_OPTIMIZATION.md` - Expected test results
- `CONTEXT_OPTIMIZATION_REQUIREMENTS_VERIFICATION.md` - Requirements verification

---

## âœ… Success Criteria

You're using this system correctly if:

- âœ… Session start <400 tokens (target: 300)
- âœ… Status checks <200 tokens (target: 150)
- âœ… Tool lookup <150 tokens (target: 100)
- âœ… Active workspace <50 markdown files
- âœ… No duplicate information (audit finds <5%)
- âœ… All files have appropriate index entries
- âœ… Progress log is append-only (never read)
- âœ… Git status shows clean (gitignored files work)

---

**Last Updated**: 2025-10-11
**Version**: 1.0
**Status**: Production - Active Use

**Remember**: The goal is to maintain 3-10K tokens per session, not to create more documentation. If in doubt, use existing structure rather than creating new files.
